{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:29:13.726644Z",
     "start_time": "2025-05-28T21:29:11.810908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# we look at path \"./eval_scores\", in which there are json files with scores\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from qq import LanguageData\n",
    "import math\n",
    "\n",
    "ld = LanguageData.from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423c17a8be3e4f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:29:13.741722600Z",
     "start_time": "2025-05-28T21:29:13.728791500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = {\"ner\": {}, \"pos\": {}, \"copa\": {}, \"qa\": {}}\n",
    "f1 = {\"ner\": \"eval_f1\", \"copa\": \"eval_acc\", \"pos\": \"eval_f1_macro\", \"qa\": \"f1\"}\n",
    "inf = math.inf\n",
    "\n",
    "\n",
    "def best_scores(scores):\n",
    "    best_scores = {}\n",
    "    for lang, types in scores.items():\n",
    "        highest = (-inf, \"None\")\n",
    "        for type, value in types.items():\n",
    "            if isinstance(value, float):\n",
    "                if value > highest[0]:\n",
    "                    highest = (value, type)\n",
    "            else:\n",
    "                for reconstructed, score in value.items():\n",
    "                    if score > highest[0]:\n",
    "                        highest = (score, reconstructed)\n",
    "\n",
    "        # print(lang, highest)\n",
    "        best_scores[lang] = highest\n",
    "    pprint(best_scores)\n",
    "    # we count how many time each type was the best\n",
    "    best_types = {}\n",
    "    for lang, (score, type) in best_scores.items():\n",
    "        if type not in best_types.keys():\n",
    "            best_types[type] = 0\n",
    "        best_types[type] += 1\n",
    "    pprint(best_types)\n",
    "\n",
    "\n",
    "for file in os.listdir(\"../eval_scores/selected\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        try:\n",
    "            with open(os.path.join(\"../eval_scores/selected\", file), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                task_name = file.split(\".\")[0]\n",
    "\n",
    "                scores[task_name] = data\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON for file: {file}\")\n",
    "        except KeyError:\n",
    "            print(\"KeyError:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1539d877b2e176",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Comparison with other papers\n",
    "\n",
    "## EMEA\n",
    "EMEA check NER and POS on quite a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74633eddf25af434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:29:13.756271600Z",
     "start_time": "2025-05-28T21:29:13.739482Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr {'baseline_en': 0.37207165824529165, 'Best': (0.5046728971962616, 'reconstructed_morphological_threshold')}\n",
      "bn {'baseline_en': 0.3659942363112392, 'Best': (0.5985275010827199, 'reconstructed_syntactic_threshold')}\n",
      "ta {'baseline_en': 0.33454252317613864, 'Best': (0.4292682926829268, 'reconstructed_featural_limit')}\n",
      "fo {'baseline_en': -inf, 'Best': (0.587360594795539, 'reconstructed_morphological_limit')}\n",
      "no {'baseline_en': 0.7269464204137571, 'Best': (0.7522368421052632, 'reconstructed_syntactic_threshold')}\n",
      "da {'baseline_en': 0.784997910572503, 'Best': (0.7937480419117904, 'reconstructed_featural_threshold')}\n",
      "be {'baseline_en': 0.5907769007062734, 'Best': (0.7273440564927423, 'reconstructed_featural_limit')}\n",
      "uk {'baseline_en': 0.5676052810476224, 'Best': (0.6025231397608616, 'reconstructed_featural')}\n",
      "bg {'baseline_en': 0.6946546253356114, 'Best': (0.7437472722999966, 'reconstructed_featural_base')}\n"
     ]
    }
   ],
   "source": [
    "# we print the highest 3 key-value pairs in a combination\n",
    "def get_highest(task, language):\n",
    "    result = {\"baseline_en\": -inf, \"Best\": (-inf, None)}\n",
    "    for type, value in scores[task][language].items():\n",
    "        # value = value*100\n",
    "        # we get the baseline of english\n",
    "        if type == \"baseline_en\":\n",
    "            result[\"baseline_en\"] = value\n",
    "        if \"baseline\" not in type:\n",
    "            if value > result[\"Best\"][0]:\n",
    "                result[\"Best\"] = (value, type)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "task = \"ner\"\n",
    "to_check = [\"mr\", \"bn\", \"ta\", \"fo\", \"no\", \"da\", \"be\", \"uk\", \"bg\"]\n",
    "\n",
    "for lang in to_check:\n",
    "    if lang in scores[task].keys():\n",
    "        print(lang, get_highest(task, lang))\n",
    "    else:\n",
    "        print(f\"{lang} not in scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d299af4635ee6de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:29:13.765350700Z",
     "start_time": "2025-05-28T21:29:13.758352300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "task2lang = {\n",
    "    \"ner\": [\"mr\", \"bn\", \"ta\", \"fo\", \"no\", \"da\", \"be\", \"uk\", \"bg\"],\n",
    "    \"pos\": [\"mr\", \"bho\", \"ta\", \"fo\", \"no\", \"da\", \"be\", \"uk\", \"bg\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "data = {\"baseline_en\": [], \"Best\": []}\n",
    "\n",
    "# Populate the dictionary with values for each language\n",
    "task = \"pos\"\n",
    "for lang in task2lang[task]:\n",
    "    if lang in scores[task].keys():\n",
    "        result = get_highest(task, lang)\n",
    "        data[\"baseline_en\"].append(result[\"baseline_en\"])\n",
    "        data[\"Best\"].append(result[\"Best\"][0])  # Append only the score from the tuple\n",
    "    else:\n",
    "        data[\"baseline_en\"].append(None)\n",
    "        data[\"Best\"].append(None)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient=\"index\", columns=to_check)\n",
    "# we add a row \"relative improvement\" which is the difference between the best and the baseline\n",
    "df.loc[\"relative improvement\"] = 1 - df.loc[\"baseline_en\"] / df.loc[\"Best\"]\n",
    "df.loc[\"absolute improvement\"] = df.loc[\"Best\"] - df.loc[\"baseline_en\"]\n",
    "# we multiply all by 100\n",
    "df = df * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5d76d5e877345",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Scores for EMEA:\n",
    "Method mr bn ta avg. fo no da avg. be uk bg avg. avg.\n",
    "En 48.0 54.4 29.6 44.0 57.5 73.3 80.5 70.4 67.1 67.6 71.1 68.6 61.0\n",
    "EMEA-s10 57.5 63.2 38.3 53.0 61.6 74.9 82.0 72.8 72.9 72.9 75.1 73.6 66.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d363d219b39c2804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:29:13.782143300Z",
     "start_time": "2025-05-28T21:29:13.761114600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mr</th>\n      <th>bn</th>\n      <th>ta</th>\n      <th>fo</th>\n      <th>no</th>\n      <th>da</th>\n      <th>be</th>\n      <th>uk</th>\n      <th>bg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline_en</th>\n      <td>48.000000</td>\n      <td>54.400000</td>\n      <td>29.600000</td>\n      <td>57.500000</td>\n      <td>73.300000</td>\n      <td>80.500000</td>\n      <td>67.100000</td>\n      <td>67.600000</td>\n      <td>71.100000</td>\n    </tr>\n    <tr>\n      <th>EMEA-s10</th>\n      <td>57.500000</td>\n      <td>63.200000</td>\n      <td>38.300000</td>\n      <td>61.600000</td>\n      <td>74.900000</td>\n      <td>82.000000</td>\n      <td>72.900000</td>\n      <td>72.900000</td>\n      <td>75.100000</td>\n    </tr>\n    <tr>\n      <th>relative improvement</th>\n      <td>0.165217</td>\n      <td>0.139241</td>\n      <td>0.227154</td>\n      <td>0.066558</td>\n      <td>0.021362</td>\n      <td>0.018293</td>\n      <td>0.079561</td>\n      <td>0.072702</td>\n      <td>0.053262</td>\n    </tr>\n    <tr>\n      <th>absolute improvement</th>\n      <td>9.500000</td>\n      <td>8.800000</td>\n      <td>8.700000</td>\n      <td>4.100000</td>\n      <td>1.600000</td>\n      <td>1.500000</td>\n      <td>5.800000</td>\n      <td>5.300000</td>\n      <td>4.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                             mr         bn         ta         fo         no  \\\nbaseline_en           48.000000  54.400000  29.600000  57.500000  73.300000   \nEMEA-s10              57.500000  63.200000  38.300000  61.600000  74.900000   \nrelative improvement   0.165217   0.139241   0.227154   0.066558   0.021362   \nabsolute improvement   9.500000   8.800000   8.700000   4.100000   1.600000   \n\n                             da         be         uk         bg  \nbaseline_en           80.500000  67.100000  67.600000  71.100000  \nEMEA-s10              82.000000  72.900000  72.900000  75.100000  \nrelative improvement   0.018293   0.079561   0.072702   0.053262  \nabsolute improvement   1.500000   5.800000   5.300000   4.000000  "
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make a dataframe with EMEA scores\n",
    "emea = {\n",
    "    \"baseline_en\": [\n",
    "        48.0,\n",
    "        54.4,\n",
    "        29.6,\n",
    "        57.5,\n",
    "        73.3,\n",
    "        80.5,\n",
    "        67.1,\n",
    "        67.6,\n",
    "        71.1,\n",
    "    ],\n",
    "    \"EMEA-s10\": [57.5, 63.2, 38.3, 61.6, 74.9, 82.0, 72.9, 72.9, 75.1],\n",
    "}\n",
    "emea_df = pd.DataFrame.from_dict(emea, orient=\"index\", columns=to_check)\n",
    "# we have to divide by 100\n",
    "emea_df.loc[\"relative improvement\"] = 1 - emea_df.loc[\"baseline_en\"] / emea_df.loc[\"EMEA-s10\"]\n",
    "emea_df.loc[\"absolute improvement\"] = emea_df.loc[\"EMEA-s10\"] - emea_df.loc[\"baseline_en\"]\n",
    "emea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fe2977ba10f103f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:29:13.803145700Z",
     "start_time": "2025-05-28T21:29:13.784198500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mr</th>\n      <th>bn</th>\n      <th>ta</th>\n      <th>fo</th>\n      <th>no</th>\n      <th>da</th>\n      <th>be</th>\n      <th>uk</th>\n      <th>bg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>emea_baseline_en</th>\n      <td>48.000000</td>\n      <td>54.400000</td>\n      <td>29.600000</td>\n      <td>57.500000</td>\n      <td>73.300000</td>\n      <td>80.500000</td>\n      <td>67.100000</td>\n      <td>67.600000</td>\n      <td>71.100000</td>\n    </tr>\n    <tr>\n      <th>EMEA-s10</th>\n      <td>57.500000</td>\n      <td>63.200000</td>\n      <td>38.300000</td>\n      <td>61.600000</td>\n      <td>74.900000</td>\n      <td>82.000000</td>\n      <td>72.900000</td>\n      <td>72.900000</td>\n      <td>75.100000</td>\n    </tr>\n    <tr>\n      <th>our_baseline_en</th>\n      <td>42.489162</td>\n      <td>33.508854</td>\n      <td>39.149474</td>\n      <td>54.998776</td>\n      <td>63.696679</td>\n      <td>77.977372</td>\n      <td>66.259707</td>\n      <td>61.913028</td>\n      <td>63.136868</td>\n    </tr>\n    <tr>\n      <th>Approximation_method</th>\n      <td>43.336831</td>\n      <td>33.885334</td>\n      <td>40.074814</td>\n      <td>57.738947</td>\n      <td>64.388078</td>\n      <td>82.669088</td>\n      <td>67.447689</td>\n      <td>62.776158</td>\n      <td>63.356007</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                             mr         bn         ta         fo         no  \\\nemea_baseline_en      48.000000  54.400000  29.600000  57.500000  73.300000   \nEMEA-s10              57.500000  63.200000  38.300000  61.600000  74.900000   \nour_baseline_en       42.489162  33.508854  39.149474  54.998776  63.696679   \nApproximation_method  43.336831  33.885334  40.074814  57.738947  64.388078   \n\n                             da         be         uk         bg  \nemea_baseline_en      80.500000  67.100000  67.600000  71.100000  \nEMEA-s10              82.000000  72.900000  72.900000  75.100000  \nour_baseline_en       77.977372  66.259707  61.913028  63.136868  \nApproximation_method  82.669088  67.447689  62.776158  63.356007  "
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we rename baseline_en index in df to \"our_baseline_en\"\n",
    "df.rename(index={\"baseline_en\": \"our_baseline_en\"}, inplace=True)\n",
    "df.rename(index={\"Best\": \"Approximation_method\"}, inplace=True)\n",
    "\n",
    "# we rename the baseline_en index in emea_df to \"emea_baseline_en\"\n",
    "emea_df.rename(index={\"baseline_en\": \"emea_baseline_en\"}, inplace=True)\n",
    "# we only take the first two columns\n",
    "emea_df = emea_df.iloc[:2, :]\n",
    "df = df.iloc[:2, :]\n",
    "# we concatenate the two dataframes\n",
    "merged_df = pd.concat([emea_df, df])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cdd59cd6fe84b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# No train but gain\n",
    "ner:\n",
    "ar bg de el es fr hi ru sw tr ur vi zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbd325c2d7138cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:29:13.803145700Z",
     "start_time": "2025-05-28T21:29:13.788697800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar {'baseline_en': 0.2433960213066638, 'Best': (0.3701895128473433, 'reconstructed_morphological_limit')}\n",
      "bg {'baseline_en': 0.6946546253356114, 'Best': (0.7437472722999966, 'reconstructed_featural_base')}\n",
      "de {'baseline_en': 0.7022521008403362, 'Best': (0.7159136884693189, 'reconstructed_syntactic_threshold')}\n",
      "el {'baseline_en': 0.6577599815192701, 'Best': (0.72478919455149, 'reconstructed_morphological_limit')}\n",
      "es {'baseline_en': 0.7115317751593586, 'Best': (0.7245094267025779, 'no_train_gain')}\n",
      "fr {'baseline_en': 0.7141884385191557, 'Best': (0.7355297017143272, 'reconstructed_syntactic_limit')}\n",
      "hi {'baseline_en': 0.5677308024158757, 'Best': (0.6572411157814291, 'reconstructed_morphological_threshold')}\n",
      "ru {'baseline_en': 0.5094573519414565, 'Best': (0.634243480258875, 'reconstructed_morphological_limit')}\n",
      "sw {'baseline_en': 0.6110886280857952, 'Best': (0.6800986842105263, 'reconstructed_morphological_threshold')}\n",
      "tr {'baseline_en': 0.5816221413364467, 'Best': (0.6042429686960127, 'reconstructed_syntactic_limit')}\n",
      "ur {'baseline_en': 0.23623853211009174, 'Best': (0.426128890837352, 'reconstructed_featural_threshold')}\n",
      "vi {'baseline_en': 0.6122260475270803, 'Best': (0.6648842087943729, 'reconstructed_syntactic_limit')}\n",
      "zh {'baseline_en': 0.1625176803394625, 'Best': (0.20129850336818364, 'reconstructed_featural_limit')}\n"
     ]
    }
   ],
   "source": [
    "# we look at the languages from no train but gain paper\n",
    "to_test = [\"ar\", \"bg\", \"de\", \"el\", \"es\", \"fr\", \"hi\", \"ru\", \"sw\", \"tr\", \"ur\", \"vi\", \"zh\"]\n",
    "# we get the scores for these languages\n",
    "for lang in to_test:\n",
    "    if lang in scores[\"ner\"].keys():\n",
    "        print(lang, get_highest(\"ner\", lang))\n",
    "    else:\n",
    "        print(f\"{lang} not in scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8d46617bced6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# QXUAD\n",
    "F1 scores:\n",
    "Model \ten \tar \tde \tel \tes \thi \tru \tth \ttr \tvi \tzh \tro \tavg\n",
    "mBERT \t83.5 \t61.5 \t70.6 \t62.6 \t75.5 \t59.2 \t71.3 \t42.7 \t55.4 \t69.5 \t58.0 \t72.7 \t65.2\n",
    "XLM-R Large \t86.5 \t68.6 \t80.4 \t79.8 \t82.0 \t76.7 \t80.1 \t74.2 \t75.9 \t79.1 \t59.3 \t83.6 \t77.2\n",
    "Translate-train mBERT \t83.5 \t68.0 \t75.6 \t70.0 \t80.2 \t69.6 \t75.0 \t36.9 \t68.9 \t75.6 \t66.2 \t- \t70.0\n",
    "Translate-test BERT-L \t87.9 \t73.7 \t79.8 \t79.4 \t82.0 \t74.9 \t79.9 \t64.6 \t67.4 \t76.3 \t73.7 \t- \t76.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea43c40a2b83e050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T22:46:48.233114700Z",
     "start_time": "2025-05-28T22:46:48.209720200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>ar</th>\n      <th>de</th>\n      <th>el</th>\n      <th>es</th>\n      <th>hi</th>\n      <th>ru</th>\n      <th>th</th>\n      <th>tr</th>\n      <th>vi</th>\n      <th>zh</th>\n      <th>ro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mBERT</th>\n      <td>83.5</td>\n      <td>61.5</td>\n      <td>70.6</td>\n      <td>62.6</td>\n      <td>75.5</td>\n      <td>59.2</td>\n      <td>71.3</td>\n      <td>42.7</td>\n      <td>55.4</td>\n      <td>69.5</td>\n      <td>58.0</td>\n      <td>72.7</td>\n    </tr>\n    <tr>\n      <th>XLM-R Large</th>\n      <td>86.5</td>\n      <td>68.6</td>\n      <td>80.4</td>\n      <td>79.8</td>\n      <td>82.0</td>\n      <td>76.7</td>\n      <td>80.1</td>\n      <td>74.2</td>\n      <td>75.9</td>\n      <td>79.1</td>\n      <td>59.3</td>\n      <td>83.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "               en    ar    de    el    es    hi    ru    th    tr    vi    zh  \\\nmBERT        83.5  61.5  70.6  62.6  75.5  59.2  71.3  42.7  55.4  69.5  58.0   \nXLM-R Large  86.5  68.6  80.4  79.8  82.0  76.7  80.1  74.2  75.9  79.1  59.3   \n\n               ro  \nmBERT        72.7  \nXLM-R Large  83.6  "
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make a dataframe out of this\n",
    "to_test = [\"en\", \"ar\", \"de\", \"el\", \"es\", \"hi\", \"ru\", \"th\", \"tr\", \"vi\", \"zh\", \"ro\"]\n",
    "qx = {\n",
    "    \"mBERT\": [83.5, 61.5, 70.6, 62.6, 75.5, 59.2, 71.3, 42.7, 55.4, 69.5, 58.0, 72.7],\n",
    "    \"XLM-R Large\": [86.5, 68.6, 80.4, 79.8, 82.0, 76.7, 80.1, 74.2, 75.9, 79.1, 59.3, 83.6],\n",
    "    # \"Translate-train mBERT\": [83.5, 68.0, 75.6, 70.0, 80.2, 69.6, 75.0, 36.9, 68.9, 75.6],\n",
    "    # \"Translate-test BERT-L\": [87.9, 73.7, 79.8, 79.4, 82.0, 74.9, 79.9, 64.6, 67.4, 76.3, 73.7],\n",
    "}\n",
    "qx_df = pd.DataFrame.from_dict(qx, orient=\"index\", columns=to_test)\n",
    "qx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ae5011773ad6571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T22:46:48.958913300Z",
     "start_time": "2025-05-28T22:46:48.928325500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>ar</th>\n      <th>de</th>\n      <th>el</th>\n      <th>es</th>\n      <th>hi</th>\n      <th>ru</th>\n      <th>th</th>\n      <th>tr</th>\n      <th>vi</th>\n      <th>zh</th>\n      <th>ro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mBERT</th>\n      <td>83.5</td>\n      <td>61.5</td>\n      <td>70.6</td>\n      <td>62.6</td>\n      <td>75.5</td>\n      <td>59.2</td>\n      <td>71.3</td>\n      <td>42.7</td>\n      <td>55.4</td>\n      <td>69.5</td>\n      <td>58.0</td>\n      <td>72.7</td>\n    </tr>\n    <tr>\n      <th>XLM-R Large</th>\n      <td>86.5</td>\n      <td>68.6</td>\n      <td>80.4</td>\n      <td>79.8</td>\n      <td>82.0</td>\n      <td>76.7</td>\n      <td>80.1</td>\n      <td>74.2</td>\n      <td>75.9</td>\n      <td>79.1</td>\n      <td>59.3</td>\n      <td>83.6</td>\n    </tr>\n    <tr>\n      <th>MAD-X</th>\n      <td>83.3</td>\n      <td>66.8</td>\n      <td>74.0</td>\n      <td>71.8</td>\n      <td>75.0</td>\n      <td>68.6</td>\n      <td>74.0</td>\n      <td>68.4</td>\n      <td>67.8</td>\n      <td>73.2</td>\n      <td>65.9</td>\n      <td>76.6</td>\n    </tr>\n    <tr>\n      <th>No Train but Gain</th>\n      <td>83.3</td>\n      <td>66.6</td>\n      <td>75.5</td>\n      <td>72.9</td>\n      <td>75.5</td>\n      <td>68.5</td>\n      <td>74.5</td>\n      <td>68.9</td>\n      <td>68.4</td>\n      <td>73.7</td>\n      <td>64.4</td>\n      <td>78.1</td>\n    </tr>\n    <tr>\n      <th>Approximation Method</th>\n      <td>83.6</td>\n      <td>67.9</td>\n      <td>76.1</td>\n      <td>73.1</td>\n      <td>75.9</td>\n      <td>69.2</td>\n      <td>75.0</td>\n      <td>69.2</td>\n      <td>69.0</td>\n      <td>73.8</td>\n      <td>66.7</td>\n      <td>78.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                        en    ar    de    el    es    hi    ru    th    tr  \\\nmBERT                 83.5  61.5  70.6  62.6  75.5  59.2  71.3  42.7  55.4   \nXLM-R Large           86.5  68.6  80.4  79.8  82.0  76.7  80.1  74.2  75.9   \nMAD-X                 83.3  66.8  74.0  71.8  75.0  68.6  74.0  68.4  67.8   \nNo Train but Gain     83.3  66.6  75.5  72.9  75.5  68.5  74.5  68.9  68.4   \nApproximation Method  83.6  67.9  76.1  73.1  75.9  69.2  75.0  69.2  69.0   \n\n                        vi    zh    ro  \nmBERT                 69.5  58.0  72.7  \nXLM-R Large           79.1  59.3  83.6  \nMAD-X                 73.2  65.9  76.6  \nNo Train but Gain     73.7  64.4  78.1  \nApproximation Method  73.8  66.7  78.9  "
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we add our scores to the dataframe\n",
    "task = \"qa\"\n",
    "for lang in to_test:\n",
    "    if lang in scores[\"qa\"].keys():\n",
    "        result = get_highest(\"qa\", lang)\n",
    "        # qx_df.loc[\"XLM-R Base\", lang] = round(scores[task][lang][\"finetune\"]*100, 1)\n",
    "        qx_df.loc[\"MAD-X\", lang] = round(scores[task][lang][\"baseline_closest_featural\"] * 100, 1)\n",
    "        # qx_df.loc[\"Approximation_method\", lang] = round(scores[task][lang][\"reconstructed_featural\"]*100, 1)\n",
    "        qx_df.loc[\"No Train but Gain\", lang] = round(scores[task][lang][\"no_train_gain\"] * 100, 1)\n",
    "        qx_df.loc[\"Approximation Method\", lang] = round(result[\"Best\"][0] * 100, 1)\n",
    "\n",
    "qx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c2b1a2f2ada7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Table to be included in the paper! qa results\n",
    "- Our method is better than finetuning mBERT, and very efficient, extendable to all languages.\n",
    "- Here we take the best approximation method, as discussed in _distance_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ecbcf1c2e9afa24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T22:46:55.737124100Z",
     "start_time": "2025-05-28T22:46:55.725593700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllllllll}\n",
      "\\toprule\n",
      " & en & ar & de & el & es & hi & ru & th & tr & vi & zh & ro \\\\\n",
      "\\midrule\n",
      "mBERT & \\tgrad[42.700][73.150][86.500]{83.5} & \\tgrad[42.700][73.150][86.500]{61.5} & \\tgrad[42.700][73.150][86.500]{70.6} & \\tgrad[42.700][73.150][86.500]{62.6} & \\tgrad[42.700][73.150][86.500]{75.5} & \\tgrad[42.700][73.150][86.500]{59.2} & \\tgrad[42.700][73.150][86.500]{71.3} & \\tgrad[42.700][73.150][86.500]{42.7} & \\tgrad[42.700][73.150][86.500]{55.4} & \\tgrad[42.700][73.150][86.500]{69.5} & \\tgrad[42.700][73.150][86.500]{58.0} & \\tgrad[42.700][73.150][86.500]{72.7} \\\\\n",
      "XLM-R Large & \\textbf{\\tgrad[42.700][73.150][86.500]{86.5}} & \\textbf{\\tgrad[42.700][73.150][86.500]{68.6}} & \\textbf{\\tgrad[42.700][73.150][86.500]{80.4}} & \\textbf{\\tgrad[42.700][73.150][86.500]{79.8}} & \\textbf{\\tgrad[42.700][73.150][86.500]{82.0}} & \\textbf{\\tgrad[42.700][73.150][86.500]{76.7}} & \\textbf{\\tgrad[42.700][73.150][86.500]{80.1}} & \\textbf{\\tgrad[42.700][73.150][86.500]{74.2}} & \\textbf{\\tgrad[42.700][73.150][86.500]{75.9}} & \\textbf{\\tgrad[42.700][73.150][86.500]{79.1}} & \\tgrad[42.700][73.150][86.500]{59.3} & \\textbf{\\tgrad[42.700][73.150][86.500]{83.6}} \\\\\n",
      "MAD-X & \\tgrad[42.700][73.150][86.500]{83.3} & \\tgrad[42.700][73.150][86.500]{66.8} & \\tgrad[42.700][73.150][86.500]{74.0} & \\tgrad[42.700][73.150][86.500]{71.8} & \\tgrad[42.700][73.150][86.500]{75.0} & \\tgrad[42.700][73.150][86.500]{68.6} & \\tgrad[42.700][73.150][86.500]{74.0} & \\tgrad[42.700][73.150][86.500]{68.4} & \\tgrad[42.700][73.150][86.500]{67.8} & \\tgrad[42.700][73.150][86.500]{73.2} & \\tgrad[42.700][73.150][86.500]{65.9} & \\tgrad[42.700][73.150][86.500]{76.6} \\\\\n",
      "No Train but Gain & \\tgrad[42.700][73.150][86.500]{83.3} & \\tgrad[42.700][73.150][86.500]{66.6} & \\tgrad[42.700][73.150][86.500]{75.5} & \\tgrad[42.700][73.150][86.500]{72.9} & \\tgrad[42.700][73.150][86.500]{75.5} & \\tgrad[42.700][73.150][86.500]{68.5} & \\tgrad[42.700][73.150][86.500]{74.5} & \\tgrad[42.700][73.150][86.500]{68.9} & \\tgrad[42.700][73.150][86.500]{68.4} & \\tgrad[42.700][73.150][86.500]{73.7} & \\tgrad[42.700][73.150][86.500]{64.4} & \\tgrad[42.700][73.150][86.500]{78.1} \\\\\n",
      "Approximation Method & \\tgrad[42.700][73.150][86.500]{83.6} & \\tgrad[42.700][73.150][86.500]{67.9} & \\tgrad[42.700][73.150][86.500]{76.1} & \\tgrad[42.700][73.150][86.500]{73.1} & \\tgrad[42.700][73.150][86.500]{75.9} & \\tgrad[42.700][73.150][86.500]{69.2} & \\tgrad[42.700][73.150][86.500]{75.0} & \\tgrad[42.700][73.150][86.500]{69.2} & \\tgrad[42.700][73.150][86.500]{69.0} & \\tgrad[42.700][73.150][86.500]{73.8} & \\textbf{\\tgrad[42.700][73.150][86.500]{66.7}} & \\tgrad[42.700][73.150][86.500]{78.9} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# we transform to Latex with the formatters etc.\n",
    "\n",
    "# 1) compute global [min, median, max] over every numeric cell\n",
    "all_vals = pd.to_numeric(qx_df.values.ravel(), errors=\"coerce\")\n",
    "mn, md, mx = np.nanmin(all_vals), float(np.nanmedian(all_vals)), np.nanmax(all_vals)\n",
    "\n",
    "# 2) record each column’s maximum (for bolding)\n",
    "col_max = qx_df.max(axis=0)\n",
    "\n",
    "# 3) build a string‐typed DataFrame, applying global gradient + bold on column‐max\n",
    "qx_str = qx_df.astype(object).copy()\n",
    "\n",
    "for idx, row in qx_df.iterrows():\n",
    "    for col in qx_df.columns:\n",
    "        x = row[col]\n",
    "        if pd.isna(x):\n",
    "            cell = \"\"\n",
    "        else:\n",
    "            # global gradient\n",
    "            grad = f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.1f}}}\"\n",
    "            # bold if it’s the max in its column\n",
    "            cell = f\"\\\\textbf{{{grad}}}\" if x == col_max[col] else grad\n",
    "        qx_str.at[idx, col] = cell\n",
    "\n",
    "# 4) export to LaTeX (letting \\tgrad and \\textbf pass through)\n",
    "latex = qx_str.to_latex(\n",
    "    escape=False,\n",
    "    multirow=True,  # keep multirow on the index if you like\n",
    ")\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146194f3fc18dcab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exact match\n",
    "Model \ten \tar \tde \tel \tes \thi \tru \tth \ttr \tvi \tzh \tro \tavg\n",
    "mBERT \t72.2 \t45.1 \t54.0 \t44.9 \t56.9 \t46.0 \t53.3 \t33.5 \t40.1 \t49.6 \t48.3 \t59.9 \t50.3\n",
    "XLM-R Large \t75.7 \t49.0 \t63.4 \t61.7 \t63.9 \t59.7 \t64.3 \t62.8 \t59.3 \t59.0 \t50.0 \t69.7 \t61.5\n",
    "Translate-train mBERT \t72.2 \t51.1 \t60.7 \t53.0 \t63.1 \t55.4 \t59.7 \t33.5 \t54.8 \t56.2 \t56.6 \t- \t56.0\n",
    "Translate-test BERT-L \t77.1 \t58.8 \t66.7 \t65.5 \t68.4 \t60.1 \t66.7 \t50.0 \t49.6 \t61.5 \t59.1 \t- \t62.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260c88bae15a7a32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:29:14.877512Z",
     "start_time": "2025-05-28T21:29:13.881134Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'improved_reconstructed_featural_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     result \u001b[38;5;241m=\u001b[39m get_highest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang)\n\u001b[0;32m     15\u001b[0m     xq_em_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mour_baseline_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang] \u001b[38;5;241m=\u001b[39m scores[task][lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline_en\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m     xq_em_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApproximation_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang] \u001b[38;5;241m=\u001b[39m scores[task][lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimproved_reconstructed_featural_all\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m     xq_em_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget language adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang] \u001b[38;5;241m=\u001b[39m scores[task][lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline_closest_featural\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'improved_reconstructed_featural_all'"
     ]
    }
   ],
   "source": [
    "xq_em = {\n",
    "    \"mBERT\": [72.2, 45.1, 54.0, 44.9, 56.9, 46.0, 53.3, 33.5, 40.1, 49.6, 48.3, 59.9],\n",
    "    \"XLM-R Large\": [75.7, 49.0, 63.4, 61.7, 63.9, 59.7, 64.3, 62.8, 59.3, 59.0, 50.0],\n",
    "    \"Translate-train mBERT\": [72.2, 51.1, 60.7, 53.0, 63.1, 55.4, 59.7, 33.5, 54.8],\n",
    "    \"Translate-test BERT-L\": [77.1, 58.8, 66.7, 65.5, 68.4, 60.1, 66.7, 50.0, 49.6, 61.5, 59.1],\n",
    "}\n",
    "\n",
    "xq_em_df = pd.DataFrame.from_dict(xq_em, orient=\"index\", columns=to_test)\n",
    "# we add our scores to the dataframe\n",
    "# we add our scores to the dataframe\n",
    "task = \"qa\"\n",
    "for lang in to_test:\n",
    "    if lang in scores[\"qa\"].keys():\n",
    "        result = get_highest(\"qa\", lang)\n",
    "        xq_em_df.loc[\"our_baseline_en\", lang] = scores[task][lang][\"baseline_en\"]\n",
    "        xq_em_df.loc[\"Approximation_method\", lang] = scores[task][lang][\"improved_reconstructed_featural_all\"]\n",
    "        xq_em_df.loc[\"Target language adapter\", lang] = scores[task][lang][\"baseline_closest_featural\"]\n",
    "    else:\n",
    "        xq_em_df.loc[\"our_baseline_en\", lang] = None\n",
    "        xq_em_df.loc[\"Approximation_method\", lang] = None\n",
    "xq_em_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23554bb5e01981d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Kunz & Holstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0db5cc69114688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T22:29:46.838792800Z",
     "start_time": "2025-05-28T22:29:46.814536200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th>Source</th>\n      <th colspan=\"2\" halign=\"left\">Kunz</th>\n      <th colspan=\"3\" halign=\"left\">Ours</th>\n    </tr>\n    <tr>\n      <th>Method</th>\n      <th>Target</th>\n      <th>English</th>\n      <th>Target</th>\n      <th>English</th>\n      <th>Approximation method</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>zh</th>\n      <td>55.2</td>\n      <td>55.0</td>\n      <td>59.2</td>\n      <td>58.8</td>\n      <td>62.0</td>\n    </tr>\n    <tr>\n      <th>vi</th>\n      <td>55.3</td>\n      <td>54.9</td>\n      <td>57.4</td>\n      <td>59.6</td>\n      <td>59.8</td>\n    </tr>\n    <tr>\n      <th>tr</th>\n      <td>53.1</td>\n      <td>51.9</td>\n      <td>54.6</td>\n      <td>53.6</td>\n      <td>59.0</td>\n    </tr>\n    <tr>\n      <th>id</th>\n      <td>55.7</td>\n      <td>53.6</td>\n      <td>60.8</td>\n      <td>57.4</td>\n      <td>59.6</td>\n    </tr>\n    <tr>\n      <th>et</th>\n      <td>54.1</td>\n      <td>50.7</td>\n      <td>55.6</td>\n      <td>52.4</td>\n      <td>58.4</td>\n    </tr>\n    <tr>\n      <th>sw</th>\n      <td>54.0</td>\n      <td>49.7</td>\n      <td>56.8</td>\n      <td>49.0</td>\n      <td>53.2</td>\n    </tr>\n    <tr>\n      <th>ht</th>\n      <td>51.2</td>\n      <td>48.6</td>\n      <td>52.4</td>\n      <td>42.0</td>\n      <td>48.6</td>\n    </tr>\n    <tr>\n      <th>qu</th>\n      <td>51.4</td>\n      <td>51.2</td>\n      <td>50.8</td>\n      <td>46.8</td>\n      <td>53.6</td>\n    </tr>\n    <tr>\n      <th>Average</th>\n      <td>53.8</td>\n      <td>52.0</td>\n      <td>56.0</td>\n      <td>52.4</td>\n      <td>56.8</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "Source    Kunz           Ours                             \nMethod  Target English Target English Approximation method\nzh        55.2    55.0   59.2    58.8                 62.0\nvi        55.3    54.9   57.4    59.6                 59.8\ntr        53.1    51.9   54.6    53.6                 59.0\nid        55.7    53.6   60.8    57.4                 59.6\net        54.1    50.7   55.6    52.4                 58.4\nsw        54.0    49.7   56.8    49.0                 53.2\nht        51.2    48.6   52.4    42.0                 48.6\nqu        51.4    51.2   50.8    46.8                 53.6\nAverage   53.8    52.0   56.0    52.4                 56.8"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data for XLM-R results on COPA\n",
    "data = {\n",
    "    \"Target\": [55.2, 55.3, 53.1, 55.7, 54.1, 54.0, 51.2, 51.4, 53.8],\n",
    "    \"English\": [55.0, 54.9, 51.9, 53.6, 50.7, 49.7, 48.6, 51.2, 52.0],\n",
    "    \"None\": [54.3, 55.1, 51.2, 53.4, 52.3, 52.0, 50.6, 49.6, 52.3],\n",
    "    \"Nonetr\": [49.4, 52.8, 49.3, 49.8, 51.4, 49.7, 49.6, 50.2, 50.3],\n",
    "}\n",
    "\n",
    "index = [\"zh\", \"vi\", \"tr\", \"id\", \"et\", \"sw\", \"ht\", \"qu\", \"Average\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_xlmr = pd.DataFrame(data, index=index)\n",
    "# we drop \"None\" and \"Nonetr\"\n",
    "df_xlmr.drop(columns=[\"None\", \"Nonetr\"], inplace=True)\n",
    "# we add a column for our COPA scores, for each of the languages\n",
    "task = \"copa\"\n",
    "for lang in index:\n",
    "    if lang in scores[task].keys():\n",
    "        result = get_highest(task, lang)\n",
    "        df_xlmr.loc[lang, \"our_target\"] = round(scores[task][lang][\"baseline_closest_featural\"], 3) * 100\n",
    "        df_xlmr.loc[lang, \"our_baseline_en\"] = round(scores[task][lang][\"baseline_en\"], 3) * 100\n",
    "        df_xlmr.loc[lang, \"Approximation_method\"] = round(result[\"Best\"][0], 3) * 100\n",
    "\n",
    "    else:\n",
    "        df_xlmr.loc[lang, \"our_baseline_en\"] = None\n",
    "        df_xlmr.loc[lang, \"Approximation_method\"] = None\n",
    "# we add the \"Average\" row for our scores\n",
    "df_xlmr.loc[\"Average\", \"our_baseline_en\"] = round(df_xlmr[\"our_baseline_en\"].mean(), 1)\n",
    "df_xlmr.loc[\"Average\", \"Approximation_method\"] = round(df_xlmr[\"Approximation_method\"].mean(), 1)\n",
    "df_xlmr.loc[\"Average\", \"our_target\"] = round(df_xlmr[\"our_target\"].mean(), 1)\n",
    "# define new MultiIndex for the columns\n",
    "df_xlmr.columns = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        (\"Kunz\", \"Target\"),\n",
    "        (\"Kunz\", \"English\"),\n",
    "        (\"Ours\", \"our_target\"),\n",
    "        (\"Ours\", \"our_baseline_en\"),\n",
    "        (\"Ours\", \"Approximation_method\"),\n",
    "    ],\n",
    "    names=[\"Source\", \"Method\"],\n",
    ")\n",
    "df_xlmr.rename(\n",
    "    columns={\"our_baseline_en\": \"English\", \"our_target\": \"Target\", \"Approximation_method\": \"Approximation method\"},\n",
    "    level=\"Method\",\n",
    "    inplace=True,\n",
    ")\n",
    "df_xlmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77113c75039f58be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T21:30:21.382793Z",
     "start_time": "2025-05-28T21:30:21.374333300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "Source & \\multicolumn{2}{r}{Kunz} & \\multicolumn{3}{r}{Ours} \\\\\n",
      "Metric & Target & English & Target & English & Approximation method \\\\\n",
      "\\midrule\n",
      "zh & \\tgrad[55.000][58.800][62.000]{55.200} & \\tgrad[55.000][58.800][62.000]{55.000} & \\tgrad[55.000][58.800][62.000]{59.200} & \\tgrad[55.000][58.800][62.000]{58.800} & \\tgrad[55.000][58.800][62.000]{62.000} \\\\\n",
      "vi & \\tgrad[54.900][57.400][59.800]{55.300} & \\tgrad[54.900][57.400][59.800]{54.900} & \\tgrad[54.900][57.400][59.800]{57.400} & \\tgrad[54.900][57.400][59.800]{59.600} & \\tgrad[54.900][57.400][59.800]{59.800} \\\\\n",
      "tr & \\tgrad[51.900][53.600][59.000]{53.100} & \\tgrad[51.900][53.600][59.000]{51.900} & \\tgrad[51.900][53.600][59.000]{54.600} & \\tgrad[51.900][53.600][59.000]{53.600} & \\tgrad[51.900][53.600][59.000]{59.000} \\\\\n",
      "id & \\tgrad[53.600][57.400][60.800]{55.700} & \\tgrad[53.600][57.400][60.800]{53.600} & \\tgrad[53.600][57.400][60.800]{60.800} & \\tgrad[53.600][57.400][60.800]{57.400} & \\tgrad[53.600][57.400][60.800]{59.600} \\\\\n",
      "et & \\tgrad[50.700][54.100][58.400]{54.100} & \\tgrad[50.700][54.100][58.400]{50.700} & \\tgrad[50.700][54.100][58.400]{55.600} & \\tgrad[50.700][54.100][58.400]{52.400} & \\tgrad[50.700][54.100][58.400]{58.400} \\\\\n",
      "sw & \\tgrad[49.000][53.200][56.800]{54.000} & \\tgrad[49.000][53.200][56.800]{49.700} & \\tgrad[49.000][53.200][56.800]{56.800} & \\tgrad[49.000][53.200][56.800]{49.000} & \\tgrad[49.000][53.200][56.800]{53.200} \\\\\n",
      "ht & \\tgrad[42.000][48.600][52.400]{51.200} & \\tgrad[42.000][48.600][52.400]{48.600} & \\tgrad[42.000][48.600][52.400]{52.400} & \\tgrad[42.000][48.600][52.400]{42.000} & \\tgrad[42.000][48.600][52.400]{48.600} \\\\\n",
      "qu & \\tgrad[46.800][51.200][53.600]{51.400} & \\tgrad[46.800][51.200][53.600]{51.200} & \\tgrad[46.800][51.200][53.600]{50.800} & \\tgrad[46.800][51.200][53.600]{46.800} & \\tgrad[46.800][51.200][53.600]{53.600} \\\\\n",
      "Average & \\tgrad[52.000][53.800][56.800]{53.800} & \\tgrad[52.000][53.800][56.800]{52.000} & \\tgrad[52.000][53.800][56.800]{56.000} & \\tgrad[52.000][53.800][56.800]{52.400} & \\tgrad[52.000][53.800][56.800]{56.800} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# 0) (Optional) escape underscores in your XLM-R columns if present\n",
    "#    — only needed if any metric name contains '_' and you want it literal in LaTeX\n",
    "import numpy as np\n",
    "\n",
    "df_for_latex = df_xlmr.copy()\n",
    "\"\"\" We want row-wise stats instead!\n",
    "# 1) Compute per-column stats on df_for_latex \n",
    "col_stats = {}\n",
    "for col in df_for_latex.columns:\n",
    "    vals = df_for_latex[col].dropna().astype(float)\n",
    "    mn, md, mx = vals.min(), float(np.median(vals)), vals.max()\n",
    "    col_stats[col] = (mn, md, mx)\n",
    "\n",
    "# 2) Build your formatters dict using exactly the same MultiIndex column keys\n",
    "formatters = {}\n",
    "for col, (mn, md, mx) in col_stats.items():\n",
    "    # default-argument trick to bind mn, md, mx at definition time\n",
    "    fmt = lambda x, mn=mn, md=md, mx=mx: (\n",
    "        f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.3f}}}\"\n",
    "        if not pd.isna(x) else \"\"\n",
    "    )\n",
    "    formatters[col] = fmt\n",
    "# 3) Export to LaTeX\n",
    "latex_table = df_for_latex.to_latex(\n",
    "    escape=False,        # let \\tgrad[...] pass through\n",
    "    formatters=formatters,\n",
    "    multirow=True\n",
    ")\n",
    "\"\"\"\n",
    "# 1) Compute per-row (min, med, max) stats\n",
    "row_stats = {\n",
    "    idx: (row.min(skipna=True), float(row.median(skipna=True)), row.max(skipna=True))\n",
    "    for idx, row in df_for_latex.astype(float).iterrows()\n",
    "}\n",
    "\n",
    "# 2) Build a new DataFrame of formatted strings\n",
    "formatted = pd.DataFrame(index=df_for_latex.index, columns=df_for_latex.columns, dtype=object)\n",
    "\n",
    "for idx in df_for_latex.index:\n",
    "    mn, md, mx = row_stats[idx]\n",
    "    for col in df_for_latex.columns:\n",
    "        x = df_for_latex.at[idx, col]\n",
    "        if pd.isna(x):\n",
    "            formatted.at[idx, col] = \"\"\n",
    "        else:\n",
    "            formatted.at[idx, col] = f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.3f}}}\"\n",
    "# 3) Export the already-formatted table to LaTeX\n",
    "latex_table = formatted.to_latex(\n",
    "    escape=False,  # our macros must pass through\n",
    "    multirow=True,  # if you still want multirow on the first index level\n",
    ")\n",
    "\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4406482da2d907f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T22:29:50.928524200Z",
     "start_time": "2025-05-28T22:29:50.919067500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "Source & \\multicolumn{2}{r}{Kunz} & \\multicolumn{3}{r}{Ours} \\\\\n",
      "Method & Target & English & Target & English & Approximation method \\\\\n",
      "\\midrule\n",
      "zh & \\tgrad[42.000][54.000][62.000]{55.200} & \\tgrad[42.000][54.000][62.000]{55.000} & \\tgrad[42.000][54.000][62.000]{59.200} & \\tgrad[42.000][54.000][62.000]{58.800} & \\tgrad[42.000][54.000][62.000]{62.000} \\\\\n",
      "vi & \\tgrad[42.000][54.000][62.000]{55.300} & \\tgrad[42.000][54.000][62.000]{54.900} & \\tgrad[42.000][54.000][62.000]{57.400} & \\tgrad[42.000][54.000][62.000]{59.600} & \\tgrad[42.000][54.000][62.000]{59.800} \\\\\n",
      "tr & \\tgrad[42.000][54.000][62.000]{53.100} & \\tgrad[42.000][54.000][62.000]{51.900} & \\tgrad[42.000][54.000][62.000]{54.600} & \\tgrad[42.000][54.000][62.000]{53.600} & \\tgrad[42.000][54.000][62.000]{59.000} \\\\\n",
      "id & \\tgrad[42.000][54.000][62.000]{55.700} & \\tgrad[42.000][54.000][62.000]{53.600} & \\tgrad[42.000][54.000][62.000]{60.800} & \\tgrad[42.000][54.000][62.000]{57.400} & \\tgrad[42.000][54.000][62.000]{59.600} \\\\\n",
      "et & \\tgrad[42.000][54.000][62.000]{54.100} & \\tgrad[42.000][54.000][62.000]{50.700} & \\tgrad[42.000][54.000][62.000]{55.600} & \\tgrad[42.000][54.000][62.000]{52.400} & \\tgrad[42.000][54.000][62.000]{58.400} \\\\\n",
      "sw & \\tgrad[42.000][54.000][62.000]{54.000} & \\tgrad[42.000][54.000][62.000]{49.700} & \\tgrad[42.000][54.000][62.000]{56.800} & \\tgrad[42.000][54.000][62.000]{49.000} & \\tgrad[42.000][54.000][62.000]{53.200} \\\\\n",
      "ht & \\tgrad[42.000][54.000][62.000]{51.200} & \\tgrad[42.000][54.000][62.000]{48.600} & \\tgrad[42.000][54.000][62.000]{52.400} & \\tgrad[42.000][54.000][62.000]{42.000} & \\tgrad[42.000][54.000][62.000]{48.600} \\\\\n",
      "qu & \\tgrad[42.000][54.000][62.000]{51.400} & \\tgrad[42.000][54.000][62.000]{51.200} & \\tgrad[42.000][54.000][62.000]{50.800} & \\tgrad[42.000][54.000][62.000]{46.800} & \\tgrad[42.000][54.000][62.000]{53.600} \\\\\n",
      "Average & \\tgrad[42.000][54.000][62.000]{53.800} & \\tgrad[42.000][54.000][62.000]{52.000} & \\tgrad[42.000][54.000][62.000]{56.000} & \\tgrad[42.000][54.000][62.000]{52.400} & \\tgrad[42.000][54.000][62.000]{56.800} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_for_latex = df_xlmr.copy()\n",
    "\n",
    "# 1) Compute global stats over all numeric cells\n",
    "all_vals = pd.to_numeric(df_for_latex.values.ravel(), errors=\"coerce\")\n",
    "mn, md, mx = np.nanmin(all_vals), np.nanmedian(all_vals), np.nanmax(all_vals)\n",
    "\n",
    "\n",
    "# 2) Build one formatter that uses the global [mn, md, mx]\n",
    "def global_fmt(x, mn=mn, md=md, mx=mx):\n",
    "    return f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.3f}}}\" if not pd.isna(x) else \"\"\n",
    "\n",
    "\n",
    "# assign the same formatter to every column\n",
    "formatters = {col: global_fmt for col in df_for_latex.columns}\n",
    "\n",
    "# 3) Export to LaTeX\n",
    "latex_table = df_for_latex.to_latex(\n",
    "    escape=False,  # let \\tgrad[...] pass through\n",
    "    formatters=formatters,\n",
    "    multirow=True,\n",
    ")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b673ecb01bd9c455",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T22:37:33.814155500Z",
     "start_time": "2025-05-28T22:37:33.810594700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "Source & \\multicolumn{2}{r}{Kunz} & \\multicolumn{3}{r}{Ours} \\\\\n",
      "Method & Target & English & Target & English & Approximation method \\\\\n",
      "\\midrule\n",
      "zh & \\tgrad[42.000][54.000][62.000]{55.200} & \\tgrad[42.000][54.000][62.000]{55.000} & \\tgrad[42.000][54.000][62.000]{59.200} & \\tgrad[42.000][54.000][62.000]{58.800} & \\textbf{\\tgrad[42.000][54.000][62.000]{62.000}} \\\\\n",
      "vi & \\tgrad[42.000][54.000][62.000]{55.300} & \\tgrad[42.000][54.000][62.000]{54.900} & \\tgrad[42.000][54.000][62.000]{57.400} & \\tgrad[42.000][54.000][62.000]{59.600} & \\textbf{\\tgrad[42.000][54.000][62.000]{59.800}} \\\\\n",
      "tr & \\tgrad[42.000][54.000][62.000]{53.100} & \\tgrad[42.000][54.000][62.000]{51.900} & \\tgrad[42.000][54.000][62.000]{54.600} & \\tgrad[42.000][54.000][62.000]{53.600} & \\textbf{\\tgrad[42.000][54.000][62.000]{59.000}} \\\\\n",
      "id & \\tgrad[42.000][54.000][62.000]{55.700} & \\tgrad[42.000][54.000][62.000]{53.600} & \\textbf{\\tgrad[42.000][54.000][62.000]{60.800}} & \\tgrad[42.000][54.000][62.000]{57.400} & \\tgrad[42.000][54.000][62.000]{59.600} \\\\\n",
      "et & \\tgrad[42.000][54.000][62.000]{54.100} & \\tgrad[42.000][54.000][62.000]{50.700} & \\tgrad[42.000][54.000][62.000]{55.600} & \\tgrad[42.000][54.000][62.000]{52.400} & \\textbf{\\tgrad[42.000][54.000][62.000]{58.400}} \\\\\n",
      "sw & \\tgrad[42.000][54.000][62.000]{54.000} & \\tgrad[42.000][54.000][62.000]{49.700} & \\textbf{\\tgrad[42.000][54.000][62.000]{56.800}} & \\tgrad[42.000][54.000][62.000]{49.000} & \\tgrad[42.000][54.000][62.000]{53.200} \\\\\n",
      "ht & \\tgrad[42.000][54.000][62.000]{51.200} & \\tgrad[42.000][54.000][62.000]{48.600} & \\textbf{\\tgrad[42.000][54.000][62.000]{52.400}} & \\tgrad[42.000][54.000][62.000]{42.000} & \\tgrad[42.000][54.000][62.000]{48.600} \\\\\n",
      "qu & \\tgrad[42.000][54.000][62.000]{51.400} & \\tgrad[42.000][54.000][62.000]{51.200} & \\tgrad[42.000][54.000][62.000]{50.800} & \\tgrad[42.000][54.000][62.000]{46.800} & \\textbf{\\tgrad[42.000][54.000][62.000]{53.600}} \\\\\n",
      "Average & \\tgrad[42.000][54.000][62.000]{53.800} & \\tgrad[42.000][54.000][62.000]{52.000} & \\tgrad[42.000][54.000][62.000]{56.000} & \\tgrad[42.000][54.000][62.000]{52.400} & \\textbf{\\tgrad[42.000][54.000][62.000]{56.800}} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_for_latex = df_xlmr.copy()\n",
    "\n",
    "# 1) Compute global stats over all numeric cells\n",
    "all_vals = pd.to_numeric(df_for_latex.values.ravel(), errors=\"coerce\")\n",
    "mn, md, mx = np.nanmin(all_vals), np.nanmedian(all_vals), np.nanmax(all_vals)\n",
    "\n",
    "# 2) Pre–compute the max score in each row\n",
    "row_max = df_for_latex.max(axis=1)\n",
    "\n",
    "# 3) Build a new “string” DataFrame, applying gradient + bold on the row‐max\n",
    "df_str = df_for_latex.astype(object).copy()\n",
    "\n",
    "for idx, row in df_for_latex.iterrows():\n",
    "    for col in df_for_latex.columns:\n",
    "        x = row[col]\n",
    "        if pd.isna(x):\n",
    "            cell = \"\"\n",
    "        else:\n",
    "            # always wrap in the global gradient\n",
    "            grad = f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.3f}}}\"\n",
    "            # bold if it’s the max in its row\n",
    "            if x == row_max.loc[idx]:\n",
    "                cell = f\"\\\\textbf{{{grad}}}\"\n",
    "            else:\n",
    "                cell = grad\n",
    "        df_str.at[idx, col] = cell\n",
    "\n",
    "# 4) Export to LaTeX\n",
    "latex_table = df_str.to_latex(\n",
    "    escape=False,  # allow \\tgrad and \\textbf through\n",
    "    multirow=True,\n",
    ")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f120ab518ebfa9ef",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
