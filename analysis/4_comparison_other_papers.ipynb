{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.420887300Z",
     "start_time": "2025-06-02T16:00:17.292138600Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# we look at path \"./eval_scores\", in which there are json files with scores\n",
    "import os\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from qq import LanguageData\n",
    "import math\n",
    "\n",
    "ld = LanguageData.from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "423c17a8be3e4f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.440032100Z",
     "start_time": "2025-06-02T16:00:19.425881600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = {\"ner\": \"eval_f1\", \"copa\": \"eval_acc\", \"pos\": \"eval_f1_macro\", \"qa\": \"f1\", \"sib\": \"eval_accuracy\"}\n",
    "tasks = f1.keys()\n",
    "scores = {task: {} for task in tasks}\n",
    "inf = math.inf\n",
    "\n",
    "\n",
    "def best_scores(scores):\n",
    "    best_scores = {}\n",
    "    for lang, types in scores.items():\n",
    "        highest = (-inf, \"None\")\n",
    "        for type, value in types.items():\n",
    "            if isinstance(value, float):\n",
    "                if value > highest[0]:\n",
    "                    highest = (value, type)\n",
    "            else:\n",
    "                for reconstructed, score in value.items():\n",
    "                    if score > highest[0]:\n",
    "                        highest = (score, reconstructed)\n",
    "\n",
    "        # print(lang, highest)\n",
    "        best_scores[lang] = highest\n",
    "    pprint(best_scores)\n",
    "    # we count how many time each type was the best\n",
    "    best_types = {}\n",
    "    for lang, (score, type) in best_scores.items():\n",
    "        if type not in best_types.keys():\n",
    "            best_types[type] = 0\n",
    "        best_types[type] += 1\n",
    "    pprint(best_types)\n",
    "\n",
    "\n",
    "for file in os.listdir(\"../eval_scores/selected\"):\n",
    "    if file.endswith(\".json\"):\n",
    "        try:\n",
    "            with open(os.path.join(\"../eval_scores/selected\", file), \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                task_name = file.split(\".\")[0]\n",
    "\n",
    "                scores[task_name] = data\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON for file: {file}\")\n",
    "        except KeyError:\n",
    "            print(\"KeyError:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1539d877b2e176",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Comparison with other papers\n",
    "\n",
    "## EMEA\n",
    "EMEA check NER and POS on quite a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74633eddf25af434",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.468735600Z",
     "start_time": "2025-06-02T16:00:19.443033600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mr {'baseline_en': 0.37207165824529165, 'Best': (0.5046728971962616, 'reconstructed_morphological_threshold')}\n",
      "bn {'baseline_en': 0.3659942363112392, 'Best': (0.5985275010827199, 'reconstructed_syntactic_threshold')}\n",
      "ta {'baseline_en': 0.33454252317613864, 'Best': (0.4292682926829268, 'reconstructed_featural_limit')}\n",
      "fo {'baseline_en': -inf, 'Best': (0.587360594795539, 'reconstructed_morphological_limit')}\n",
      "no {'baseline_en': 0.7269464204137571, 'Best': (0.7522368421052632, 'reconstructed_syntactic_threshold')}\n",
      "da {'baseline_en': 0.784997910572503, 'Best': (0.7937480419117904, 'reconstructed_featural_threshold')}\n",
      "be {'baseline_en': 0.5907769007062734, 'Best': (0.7273440564927423, 'reconstructed_featural_limit')}\n",
      "uk {'baseline_en': 0.5676052810476224, 'Best': (0.6025231397608616, 'reconstructed_featural')}\n",
      "bg {'baseline_en': 0.6946546253356114, 'Best': (0.7437472722999966, 'reconstructed_featural_base')}\n"
     ]
    }
   ],
   "source": [
    "# we print the highest 3 key-value pairs in a combination\n",
    "def get_highest(task, language):\n",
    "    result = {\"baseline_en\": -inf, \"Best\": (-inf, None)}\n",
    "    for type, value in scores[task][language].items():\n",
    "        # value = value*100\n",
    "        # we get the baseline of english\n",
    "        if type == \"baseline_en\":\n",
    "            result[\"baseline_en\"] = value\n",
    "        if \"baseline\" not in type:\n",
    "            if value > result[\"Best\"][0]:\n",
    "                result[\"Best\"] = (value, type)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "task = \"ner\"\n",
    "to_check = [\"mr\", \"bn\", \"ta\", \"fo\", \"no\", \"da\", \"be\", \"uk\", \"bg\"]\n",
    "\n",
    "for lang in to_check:\n",
    "    if lang in scores[task].keys():\n",
    "        print(lang, get_highest(task, lang))\n",
    "    else:\n",
    "        print(f\"{lang} not in scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d299af4635ee6de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.470269400Z",
     "start_time": "2025-06-02T16:00:19.451825300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "task2lang = {\n",
    "    \"ner\": [\"mr\", \"bn\", \"ta\", \"fo\", \"no\", \"da\", \"be\", \"uk\", \"bg\"],\n",
    "    \"pos\": [\"mr\", \"bho\", \"ta\", \"fo\", \"no\", \"da\", \"be\", \"uk\", \"bg\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "data = {\"baseline_en\": [], \"Best\": []}\n",
    "\n",
    "# Populate the dictionary with values for each language\n",
    "task = \"pos\"\n",
    "for lang in task2lang[task]:\n",
    "    if lang in scores[task].keys():\n",
    "        result = get_highest(task, lang)\n",
    "        data[\"baseline_en\"].append(result[\"baseline_en\"])\n",
    "        data[\"Best\"].append(result[\"Best\"][0])  # Append only the score from the tuple\n",
    "    else:\n",
    "        data[\"baseline_en\"].append(None)\n",
    "        data[\"Best\"].append(None)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient=\"index\", columns=to_check)\n",
    "# we add a row \"relative improvement\" which is the difference between the best and the baseline\n",
    "df.loc[\"relative improvement\"] = 1 - df.loc[\"baseline_en\"] / df.loc[\"Best\"]\n",
    "df.loc[\"absolute improvement\"] = df.loc[\"Best\"] - df.loc[\"baseline_en\"]\n",
    "# we multiply all by 100\n",
    "df = df * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5d76d5e877345",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Scores for EMEA:\n",
    "Method mr bn ta avg. fo no da avg. be uk bg avg. avg.\n",
    "En 48.0 54.4 29.6 44.0 57.5 73.3 80.5 70.4 67.1 67.6 71.1 68.6 61.0\n",
    "EMEA-s10 57.5 63.2 38.3 53.0 61.6 74.9 82.0 72.8 72.9 72.9 75.1 73.6 66.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d363d219b39c2804",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.484436900Z",
     "start_time": "2025-06-02T16:00:19.460021700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mr</th>\n      <th>bn</th>\n      <th>ta</th>\n      <th>fo</th>\n      <th>no</th>\n      <th>da</th>\n      <th>be</th>\n      <th>uk</th>\n      <th>bg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline_en</th>\n      <td>48.000000</td>\n      <td>54.400000</td>\n      <td>29.600000</td>\n      <td>57.500000</td>\n      <td>73.300000</td>\n      <td>80.500000</td>\n      <td>67.100000</td>\n      <td>67.600000</td>\n      <td>71.100000</td>\n    </tr>\n    <tr>\n      <th>EMEA-s10</th>\n      <td>57.500000</td>\n      <td>63.200000</td>\n      <td>38.300000</td>\n      <td>61.600000</td>\n      <td>74.900000</td>\n      <td>82.000000</td>\n      <td>72.900000</td>\n      <td>72.900000</td>\n      <td>75.100000</td>\n    </tr>\n    <tr>\n      <th>relative improvement</th>\n      <td>0.165217</td>\n      <td>0.139241</td>\n      <td>0.227154</td>\n      <td>0.066558</td>\n      <td>0.021362</td>\n      <td>0.018293</td>\n      <td>0.079561</td>\n      <td>0.072702</td>\n      <td>0.053262</td>\n    </tr>\n    <tr>\n      <th>absolute improvement</th>\n      <td>9.500000</td>\n      <td>8.800000</td>\n      <td>8.700000</td>\n      <td>4.100000</td>\n      <td>1.600000</td>\n      <td>1.500000</td>\n      <td>5.800000</td>\n      <td>5.300000</td>\n      <td>4.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                             mr         bn         ta         fo         no  \\\nbaseline_en           48.000000  54.400000  29.600000  57.500000  73.300000   \nEMEA-s10              57.500000  63.200000  38.300000  61.600000  74.900000   \nrelative improvement   0.165217   0.139241   0.227154   0.066558   0.021362   \nabsolute improvement   9.500000   8.800000   8.700000   4.100000   1.600000   \n\n                             da         be         uk         bg  \nbaseline_en           80.500000  67.100000  67.600000  71.100000  \nEMEA-s10              82.000000  72.900000  72.900000  75.100000  \nrelative improvement   0.018293   0.079561   0.072702   0.053262  \nabsolute improvement   1.500000   5.800000   5.300000   4.000000  "
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make a dataframe with EMEA scores\n",
    "emea = {\n",
    "    \"baseline_en\": [\n",
    "        48.0,\n",
    "        54.4,\n",
    "        29.6,\n",
    "        57.5,\n",
    "        73.3,\n",
    "        80.5,\n",
    "        67.1,\n",
    "        67.6,\n",
    "        71.1,\n",
    "    ],\n",
    "    \"EMEA-s10\": [57.5, 63.2, 38.3, 61.6, 74.9, 82.0, 72.9, 72.9, 75.1],\n",
    "}\n",
    "emea_df = pd.DataFrame.from_dict(emea, orient=\"index\", columns=to_check)\n",
    "# we have to divide by 100\n",
    "emea_df.loc[\"relative improvement\"] = 1 - emea_df.loc[\"baseline_en\"] / emea_df.loc[\"EMEA-s10\"]\n",
    "emea_df.loc[\"absolute improvement\"] = emea_df.loc[\"EMEA-s10\"] - emea_df.loc[\"baseline_en\"]\n",
    "emea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fe2977ba10f103f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.485987100Z",
     "start_time": "2025-06-02T16:00:19.479231200Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mr</th>\n      <th>bn</th>\n      <th>ta</th>\n      <th>fo</th>\n      <th>no</th>\n      <th>da</th>\n      <th>be</th>\n      <th>uk</th>\n      <th>bg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>emea_baseline_en</th>\n      <td>48.000000</td>\n      <td>54.400000</td>\n      <td>29.600000</td>\n      <td>57.500000</td>\n      <td>73.300000</td>\n      <td>80.500000</td>\n      <td>67.100000</td>\n      <td>67.600000</td>\n      <td>71.100000</td>\n    </tr>\n    <tr>\n      <th>EMEA-s10</th>\n      <td>57.500000</td>\n      <td>63.200000</td>\n      <td>38.300000</td>\n      <td>61.600000</td>\n      <td>74.900000</td>\n      <td>82.000000</td>\n      <td>72.900000</td>\n      <td>72.900000</td>\n      <td>75.100000</td>\n    </tr>\n    <tr>\n      <th>our_baseline_en</th>\n      <td>42.489162</td>\n      <td>33.508854</td>\n      <td>39.149474</td>\n      <td>54.998776</td>\n      <td>63.696679</td>\n      <td>77.977372</td>\n      <td>66.259707</td>\n      <td>61.913028</td>\n      <td>63.136868</td>\n    </tr>\n    <tr>\n      <th>Approximation_method</th>\n      <td>43.336831</td>\n      <td>33.885334</td>\n      <td>40.074814</td>\n      <td>57.738947</td>\n      <td>64.388078</td>\n      <td>82.669088</td>\n      <td>67.447689</td>\n      <td>62.776158</td>\n      <td>63.356007</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                             mr         bn         ta         fo         no  \\\nemea_baseline_en      48.000000  54.400000  29.600000  57.500000  73.300000   \nEMEA-s10              57.500000  63.200000  38.300000  61.600000  74.900000   \nour_baseline_en       42.489162  33.508854  39.149474  54.998776  63.696679   \nApproximation_method  43.336831  33.885334  40.074814  57.738947  64.388078   \n\n                             da         be         uk         bg  \nemea_baseline_en      80.500000  67.100000  67.600000  71.100000  \nEMEA-s10              82.000000  72.900000  72.900000  75.100000  \nour_baseline_en       77.977372  66.259707  61.913028  63.136868  \nApproximation_method  82.669088  67.447689  62.776158  63.356007  "
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we rename baseline_en index in df to \"our_baseline_en\"\n",
    "df.rename(index={\"baseline_en\": \"our_baseline_en\"}, inplace=True)\n",
    "df.rename(index={\"Best\": \"Approximation_method\"}, inplace=True)\n",
    "\n",
    "# we rename the baseline_en index in emea_df to \"emea_baseline_en\"\n",
    "emea_df.rename(index={\"baseline_en\": \"emea_baseline_en\"}, inplace=True)\n",
    "# we only take the first two columns\n",
    "emea_df = emea_df.iloc[:2, :]\n",
    "df = df.iloc[:2, :]\n",
    "# we concatenate the two dataframes\n",
    "merged_df = pd.concat([emea_df, df])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cdd59cd6fe84b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# No train but gain\n",
    "ner:\n",
    "ar bg de el es fr hi ru sw tr ur vi zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bbd325c2d7138cb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.514133200Z",
     "start_time": "2025-06-02T16:00:19.482781600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar {'baseline_en': 0.2433960213066638, 'Best': (0.3701895128473433, 'reconstructed_morphological_limit')}\n",
      "bg {'baseline_en': 0.6946546253356114, 'Best': (0.7437472722999966, 'reconstructed_featural_base')}\n",
      "de {'baseline_en': 0.7022521008403362, 'Best': (0.7159136884693189, 'reconstructed_syntactic_threshold')}\n",
      "el {'baseline_en': 0.6577599815192701, 'Best': (0.72478919455149, 'reconstructed_morphological_limit')}\n",
      "es {'baseline_en': 0.7115317751593586, 'Best': (0.7245094267025779, 'no_train_gain')}\n",
      "fr {'baseline_en': 0.7141884385191557, 'Best': (0.7355297017143272, 'reconstructed_syntactic_limit')}\n",
      "hi {'baseline_en': 0.5677308024158757, 'Best': (0.6572411157814291, 'reconstructed_morphological_threshold')}\n",
      "ru {'baseline_en': 0.5094573519414565, 'Best': (0.634243480258875, 'reconstructed_morphological_limit')}\n",
      "sw {'baseline_en': 0.6110886280857952, 'Best': (0.6800986842105263, 'reconstructed_morphological_threshold')}\n",
      "tr {'baseline_en': 0.5816221413364467, 'Best': (0.6042429686960127, 'reconstructed_syntactic_limit')}\n",
      "ur {'baseline_en': 0.23623853211009174, 'Best': (0.426128890837352, 'reconstructed_featural_threshold')}\n",
      "vi {'baseline_en': 0.6122260475270803, 'Best': (0.6648842087943729, 'reconstructed_syntactic_limit')}\n",
      "zh {'baseline_en': 0.1625176803394625, 'Best': (0.20129850336818364, 'reconstructed_featural_limit')}\n"
     ]
    }
   ],
   "source": [
    "# we look at the languages from no train but gain paper\n",
    "to_test = [\"ar\", \"bg\", \"de\", \"el\", \"es\", \"fr\", \"hi\", \"ru\", \"sw\", \"tr\", \"ur\", \"vi\", \"zh\"]\n",
    "# we get the scores for these languages\n",
    "for lang in to_test:\n",
    "    if lang in scores[\"ner\"].keys():\n",
    "        print(lang, get_highest(\"ner\", lang))\n",
    "    else:\n",
    "        print(f\"{lang} not in scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8d46617bced6e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# QXUAD\n",
    "F1 scores:\n",
    "Model \ten \tar \tde \tel \tes \thi \tru \tth \ttr \tvi \tzh \tro \tavg\n",
    "mBERT \t83.5 \t61.5 \t70.6 \t62.6 \t75.5 \t59.2 \t71.3 \t42.7 \t55.4 \t69.5 \t58.0 \t72.7 \t65.2\n",
    "XLM-R Large \t86.5 \t68.6 \t80.4 \t79.8 \t82.0 \t76.7 \t80.1 \t74.2 \t75.9 \t79.1 \t59.3 \t83.6 \t77.2\n",
    "Translate-train mBERT \t83.5 \t68.0 \t75.6 \t70.0 \t80.2 \t69.6 \t75.0 \t36.9 \t68.9 \t75.6 \t66.2 \t- \t70.0\n",
    "Translate-test BERT-L \t87.9 \t73.7 \t79.8 \t79.4 \t82.0 \t74.9 \t79.9 \t64.6 \t67.4 \t76.3 \t73.7 \t- \t76.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea43c40a2b83e050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.515133800Z",
     "start_time": "2025-06-02T16:00:19.489029600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>ar</th>\n      <th>de</th>\n      <th>el</th>\n      <th>es</th>\n      <th>hi</th>\n      <th>ru</th>\n      <th>th</th>\n      <th>tr</th>\n      <th>vi</th>\n      <th>zh</th>\n      <th>ro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mBERT</th>\n      <td>83.5</td>\n      <td>61.5</td>\n      <td>70.6</td>\n      <td>62.6</td>\n      <td>75.5</td>\n      <td>59.2</td>\n      <td>71.3</td>\n      <td>42.7</td>\n      <td>55.4</td>\n      <td>69.5</td>\n      <td>58.0</td>\n      <td>72.7</td>\n    </tr>\n    <tr>\n      <th>XLM-R Large</th>\n      <td>86.5</td>\n      <td>68.6</td>\n      <td>80.4</td>\n      <td>79.8</td>\n      <td>82.0</td>\n      <td>76.7</td>\n      <td>80.1</td>\n      <td>74.2</td>\n      <td>75.9</td>\n      <td>79.1</td>\n      <td>59.3</td>\n      <td>83.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "               en    ar    de    el    es    hi    ru    th    tr    vi    zh  \\\nmBERT        83.5  61.5  70.6  62.6  75.5  59.2  71.3  42.7  55.4  69.5  58.0   \nXLM-R Large  86.5  68.6  80.4  79.8  82.0  76.7  80.1  74.2  75.9  79.1  59.3   \n\n               ro  \nmBERT        72.7  \nXLM-R Large  83.6  "
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make a dataframe out of this\n",
    "to_test = [\"en\", \"ar\", \"de\", \"el\", \"es\", \"hi\", \"ru\", \"th\", \"tr\", \"vi\", \"zh\", \"ro\"]\n",
    "qx = {\n",
    "    \"mBERT\": [83.5, 61.5, 70.6, 62.6, 75.5, 59.2, 71.3, 42.7, 55.4, 69.5, 58.0, 72.7],\n",
    "    \"XLM-R Large\": [86.5, 68.6, 80.4, 79.8, 82.0, 76.7, 80.1, 74.2, 75.9, 79.1, 59.3, 83.6],\n",
    "    # \"Translate-train mBERT\": [83.5, 68.0, 75.6, 70.0, 80.2, 69.6, 75.0, 36.9, 68.9, 75.6],\n",
    "    # \"Translate-test BERT-L\": [87.9, 73.7, 79.8, 79.4, 82.0, 74.9, 79.9, 64.6, 67.4, 76.3, 73.7],\n",
    "}\n",
    "qx_df = pd.DataFrame.from_dict(qx, orient=\"index\", columns=to_test)\n",
    "qx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ae5011773ad6571",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.571347500Z",
     "start_time": "2025-06-02T16:00:19.509305400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>ar</th>\n      <th>de</th>\n      <th>el</th>\n      <th>es</th>\n      <th>hi</th>\n      <th>ru</th>\n      <th>th</th>\n      <th>tr</th>\n      <th>vi</th>\n      <th>zh</th>\n      <th>ro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>mBERT</th>\n      <td>83.5</td>\n      <td>61.5</td>\n      <td>70.6</td>\n      <td>62.6</td>\n      <td>75.5</td>\n      <td>59.2</td>\n      <td>71.3</td>\n      <td>42.7</td>\n      <td>55.4</td>\n      <td>69.5</td>\n      <td>58.0</td>\n      <td>72.7</td>\n    </tr>\n    <tr>\n      <th>XLM-R Large</th>\n      <td>86.5</td>\n      <td>68.6</td>\n      <td>80.4</td>\n      <td>79.8</td>\n      <td>82.0</td>\n      <td>76.7</td>\n      <td>80.1</td>\n      <td>74.2</td>\n      <td>75.9</td>\n      <td>79.1</td>\n      <td>59.3</td>\n      <td>83.6</td>\n    </tr>\n    <tr>\n      <th>MAD-X</th>\n      <td>83.3</td>\n      <td>66.8</td>\n      <td>74.0</td>\n      <td>71.8</td>\n      <td>75.0</td>\n      <td>68.6</td>\n      <td>74.0</td>\n      <td>68.4</td>\n      <td>67.8</td>\n      <td>73.2</td>\n      <td>65.9</td>\n      <td>76.6</td>\n    </tr>\n    <tr>\n      <th>No Train but Gain</th>\n      <td>83.3</td>\n      <td>66.6</td>\n      <td>75.5</td>\n      <td>72.9</td>\n      <td>75.5</td>\n      <td>68.5</td>\n      <td>74.5</td>\n      <td>68.9</td>\n      <td>68.4</td>\n      <td>73.7</td>\n      <td>64.4</td>\n      <td>78.1</td>\n    </tr>\n    <tr>\n      <th>TIPA</th>\n      <td>83.6</td>\n      <td>67.9</td>\n      <td>76.1</td>\n      <td>73.1</td>\n      <td>75.9</td>\n      <td>69.2</td>\n      <td>75.0</td>\n      <td>69.2</td>\n      <td>69.0</td>\n      <td>73.8</td>\n      <td>66.7</td>\n      <td>78.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                     en    ar    de    el    es    hi    ru    th    tr    vi  \\\nmBERT              83.5  61.5  70.6  62.6  75.5  59.2  71.3  42.7  55.4  69.5   \nXLM-R Large        86.5  68.6  80.4  79.8  82.0  76.7  80.1  74.2  75.9  79.1   \nMAD-X              83.3  66.8  74.0  71.8  75.0  68.6  74.0  68.4  67.8  73.2   \nNo Train but Gain  83.3  66.6  75.5  72.9  75.5  68.5  74.5  68.9  68.4  73.7   \nTIPA               83.6  67.9  76.1  73.1  75.9  69.2  75.0  69.2  69.0  73.8   \n\n                     zh    ro  \nmBERT              58.0  72.7  \nXLM-R Large        59.3  83.6  \nMAD-X              65.9  76.6  \nNo Train but Gain  64.4  78.1  \nTIPA               66.7  78.9  "
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we add our scores to the dataframe\n",
    "task = \"qa\"\n",
    "for lang in to_test:\n",
    "    if lang in scores[\"qa\"].keys():\n",
    "        result = get_highest(\"qa\", lang)\n",
    "        # qx_df.loc[\"XLM-R Base\", lang] = round(scores[task][lang][\"finetune\"]*100, 1)\n",
    "        qx_df.loc[\"MAD-X\", lang] = round(scores[task][lang][\"baseline_closest_featural\"] * 100, 1)\n",
    "        # qx_df.loc[\"Approximation_method\", lang] = round(scores[task][lang][\"reconstructed_featural\"]*100, 1)\n",
    "        qx_df.loc[\"No Train but Gain\", lang] = round(scores[task][lang][\"no_train_gain\"] * 100, 1)\n",
    "        qx_df.loc[\"TIPA\", lang] = round(result[\"Best\"][0] * 100, 1)\n",
    "\n",
    "qx_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c2b1a2f2ada7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Table to be included in the paper! qa results\n",
    "- Our method is better than finetuning mBERT, and very efficient, extendable to all languages.\n",
    "- Here we take the best approximation method, as discussed in _distance_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ecbcf1c2e9afa24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.639471600Z",
     "start_time": "2025-06-02T16:00:19.522794500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllllllll}\n",
      "\\toprule\n",
      " & en & ar & de & el & es & hi & ru & th & tr & vi & zh & ro \\\\\n",
      "\\midrule\n",
      "mBERT & \\tgrad[42.700][73.150][86.500]{83.5} & \\tgrad[42.700][73.150][86.500]{61.5} & \\tgrad[42.700][73.150][86.500]{70.6} & \\tgrad[42.700][73.150][86.500]{62.6} & \\tgrad[42.700][73.150][86.500]{75.5} & \\tgrad[42.700][73.150][86.500]{59.2} & \\tgrad[42.700][73.150][86.500]{71.3} & \\tgrad[42.700][73.150][86.500]{42.7} & \\tgrad[42.700][73.150][86.500]{55.4} & \\tgrad[42.700][73.150][86.500]{69.5} & \\tgrad[42.700][73.150][86.500]{58.0} & \\tgrad[42.700][73.150][86.500]{72.7} \\\\\n",
      "XLM-R Large & \\textbf{\\tgrad[42.700][73.150][86.500]{86.5}} & \\textbf{\\tgrad[42.700][73.150][86.500]{68.6}} & \\textbf{\\tgrad[42.700][73.150][86.500]{80.4}} & \\textbf{\\tgrad[42.700][73.150][86.500]{79.8}} & \\textbf{\\tgrad[42.700][73.150][86.500]{82.0}} & \\textbf{\\tgrad[42.700][73.150][86.500]{76.7}} & \\textbf{\\tgrad[42.700][73.150][86.500]{80.1}} & \\textbf{\\tgrad[42.700][73.150][86.500]{74.2}} & \\textbf{\\tgrad[42.700][73.150][86.500]{75.9}} & \\textbf{\\tgrad[42.700][73.150][86.500]{79.1}} & \\tgrad[42.700][73.150][86.500]{59.3} & \\textbf{\\tgrad[42.700][73.150][86.500]{83.6}} \\\\\n",
      "MAD-X & \\tgrad[42.700][73.150][86.500]{83.3} & \\tgrad[42.700][73.150][86.500]{66.8} & \\tgrad[42.700][73.150][86.500]{74.0} & \\tgrad[42.700][73.150][86.500]{71.8} & \\tgrad[42.700][73.150][86.500]{75.0} & \\tgrad[42.700][73.150][86.500]{68.6} & \\tgrad[42.700][73.150][86.500]{74.0} & \\tgrad[42.700][73.150][86.500]{68.4} & \\tgrad[42.700][73.150][86.500]{67.8} & \\tgrad[42.700][73.150][86.500]{73.2} & \\tgrad[42.700][73.150][86.500]{65.9} & \\tgrad[42.700][73.150][86.500]{76.6} \\\\\n",
      "No Train but Gain & \\tgrad[42.700][73.150][86.500]{83.3} & \\tgrad[42.700][73.150][86.500]{66.6} & \\tgrad[42.700][73.150][86.500]{75.5} & \\tgrad[42.700][73.150][86.500]{72.9} & \\tgrad[42.700][73.150][86.500]{75.5} & \\tgrad[42.700][73.150][86.500]{68.5} & \\tgrad[42.700][73.150][86.500]{74.5} & \\tgrad[42.700][73.150][86.500]{68.9} & \\tgrad[42.700][73.150][86.500]{68.4} & \\tgrad[42.700][73.150][86.500]{73.7} & \\tgrad[42.700][73.150][86.500]{64.4} & \\tgrad[42.700][73.150][86.500]{78.1} \\\\\n",
      "TIPA & \\tgrad[42.700][73.150][86.500]{83.6} & \\tgrad[42.700][73.150][86.500]{67.9} & \\tgrad[42.700][73.150][86.500]{76.1} & \\tgrad[42.700][73.150][86.500]{73.1} & \\tgrad[42.700][73.150][86.500]{75.9} & \\tgrad[42.700][73.150][86.500]{69.2} & \\tgrad[42.700][73.150][86.500]{75.0} & \\tgrad[42.700][73.150][86.500]{69.2} & \\tgrad[42.700][73.150][86.500]{69.0} & \\tgrad[42.700][73.150][86.500]{73.8} & \\textbf{\\tgrad[42.700][73.150][86.500]{66.7}} & \\tgrad[42.700][73.150][86.500]{78.9} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# we transform to Latex with the formatters etc.\n",
    "\n",
    "# 1) compute global [min, median, max] over every numeric cell\n",
    "all_vals = pd.to_numeric(qx_df.values.ravel(), errors=\"coerce\")\n",
    "mn, md, mx = np.nanmin(all_vals), float(np.nanmedian(all_vals)), np.nanmax(all_vals)\n",
    "\n",
    "# 2) record each column’s maximum (for bolding)\n",
    "col_max = qx_df.max(axis=0)\n",
    "\n",
    "# 3) build a string‐typed DataFrame, applying global gradient + bold on column‐max\n",
    "qx_str = qx_df.astype(object).copy()\n",
    "\n",
    "for idx, row in qx_df.iterrows():\n",
    "    for col in qx_df.columns:\n",
    "        x = row[col]\n",
    "        if pd.isna(x):\n",
    "            cell = \"\"\n",
    "        else:\n",
    "            # global gradient\n",
    "            grad = f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.1f}}}\"\n",
    "            # bold if it’s the max in its column\n",
    "            cell = f\"\\\\textbf{{{grad}}}\" if x == col_max[col] else grad\n",
    "        qx_str.at[idx, col] = cell\n",
    "\n",
    "# 4) export to LaTeX (letting \\tgrad and \\textbf pass through)\n",
    "latex = qx_str.to_latex(\n",
    "    escape=False,\n",
    "    multirow=True,  # keep multirow on the index if you like\n",
    ")\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146194f3fc18dcab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exact match\n",
    "Model \ten \tar \tde \tel \tes \thi \tru \tth \ttr \tvi \tzh \tro \tavg\n",
    "mBERT \t72.2 \t45.1 \t54.0 \t44.9 \t56.9 \t46.0 \t53.3 \t33.5 \t40.1 \t49.6 \t48.3 \t59.9 \t50.3\n",
    "XLM-R Large \t75.7 \t49.0 \t63.4 \t61.7 \t63.9 \t59.7 \t64.3 \t62.8 \t59.3 \t59.0 \t50.0 \t69.7 \t61.5\n",
    "Translate-train mBERT \t72.2 \t51.1 \t60.7 \t53.0 \t63.1 \t55.4 \t59.7 \t33.5 \t54.8 \t56.2 \t56.6 \t- \t56.0\n",
    "Translate-test BERT-L \t77.1 \t58.8 \t66.7 \t65.5 \t68.4 \t60.1 \t66.7 \t50.0 \t49.6 \t61.5 \t59.1 \t- \t62.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "260c88bae15a7a32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:00:19.676097500Z",
     "start_time": "2025-06-02T16:00:19.527801300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'improved_reconstructed_featural_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m     result \u001b[38;5;241m=\u001b[39m get_highest(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqa\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang)\n\u001b[0;32m     15\u001b[0m     xq_em_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mour_baseline_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang] \u001b[38;5;241m=\u001b[39m scores[task][lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline_en\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 16\u001b[0m     xq_em_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApproximation_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang] \u001b[38;5;241m=\u001b[39m scores[task][lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimproved_reconstructed_featural_all\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     17\u001b[0m     xq_em_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget language adapter\u001b[39m\u001b[38;5;124m\"\u001b[39m, lang] \u001b[38;5;241m=\u001b[39m scores[task][lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline_closest_featural\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'improved_reconstructed_featural_all'"
     ]
    }
   ],
   "source": [
    "xq_em = {\n",
    "    \"mBERT\": [72.2, 45.1, 54.0, 44.9, 56.9, 46.0, 53.3, 33.5, 40.1, 49.6, 48.3, 59.9],\n",
    "    \"XLM-R Large\": [75.7, 49.0, 63.4, 61.7, 63.9, 59.7, 64.3, 62.8, 59.3, 59.0, 50.0],\n",
    "    \"Translate-train mBERT\": [72.2, 51.1, 60.7, 53.0, 63.1, 55.4, 59.7, 33.5, 54.8],\n",
    "    \"Translate-test BERT-L\": [77.1, 58.8, 66.7, 65.5, 68.4, 60.1, 66.7, 50.0, 49.6, 61.5, 59.1],\n",
    "}\n",
    "\n",
    "xq_em_df = pd.DataFrame.from_dict(xq_em, orient=\"index\", columns=to_test)\n",
    "# we add our scores to the dataframe\n",
    "# we add our scores to the dataframe\n",
    "task = \"qa\"\n",
    "for lang in to_test:\n",
    "    if lang in scores[\"qa\"].keys():\n",
    "        result = get_highest(\"qa\", lang)\n",
    "        xq_em_df.loc[\"our_baseline_en\", lang] = scores[task][lang][\"baseline_en\"]\n",
    "        xq_em_df.loc[\"Approximation_method\", lang] = scores[task][lang][\"improved_reconstructed_featural_all\"]\n",
    "        xq_em_df.loc[\"Target language adapter\", lang] = scores[task][lang][\"baseline_closest_featural\"]\n",
    "    else:\n",
    "        xq_em_df.loc[\"our_baseline_en\", lang] = None\n",
    "        xq_em_df.loc[\"Approximation_method\", lang] = None\n",
    "xq_em_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23554bb5e01981d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Kunz & Holstrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0db5cc69114688",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-02T16:00:19.554182800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data for XLM-R results on COPA\n",
    "data = {\n",
    "    \"Target adapter\": [55.2, 55.3, 53.1, 55.7, 54.1, 54.0, 51.2, 51.4, 53.8],\n",
    "    \"English adapter\": [55.0, 54.9, 51.9, 53.6, 50.7, 49.7, 48.6, 51.2, 52.0],\n",
    "    \"None\": [54.3, 55.1, 51.2, 53.4, 52.3, 52.0, 50.6, 49.6, 52.3],\n",
    "    \"Nonetr\": [49.4, 52.8, 49.3, 49.8, 51.4, 49.7, 49.6, 50.2, 50.3],\n",
    "}\n",
    "\n",
    "index = [\"zh\", \"vi\", \"tr\", \"id\", \"et\", \"sw\", \"ht\", \"qu\", \"Average\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df_xlmr = pd.DataFrame(data, index=index)\n",
    "# we drop \"None\" and \"Nonetr\"\n",
    "df_xlmr.drop(columns=[\"None\", \"Nonetr\"], inplace=True)\n",
    "# we add a column for our COPA scores, for each of the languages\n",
    "task = \"copa\"\n",
    "for lang in index:\n",
    "    if lang in scores[task].keys():\n",
    "        result = get_highest(task, lang)\n",
    "        df_xlmr.loc[lang, \"our_target\"] = round(scores[task][lang][\"baseline_closest_featural\"], 3) * 100\n",
    "        df_xlmr.loc[lang, \"our_baseline_en\"] = round(scores[task][lang][\"baseline_en\"], 3) * 100\n",
    "        df_xlmr.loc[lang, \"Approximation_method\"] = round(result[\"Best\"][0], 3) * 100\n",
    "\n",
    "    else:\n",
    "        df_xlmr.loc[lang, \"our_baseline_en\"] = None\n",
    "        df_xlmr.loc[lang, \"Approximation_method\"] = None\n",
    "# we add the \"Average\" row for our scores\n",
    "df_xlmr.loc[\"Average\", \"our_baseline_en\"] = round(df_xlmr[\"our_baseline_en\"].mean(), 1)\n",
    "df_xlmr.loc[\"Average\", \"Approximation_method\"] = round(df_xlmr[\"Approximation_method\"].mean(), 1)\n",
    "df_xlmr.loc[\"Average\", \"our_target\"] = round(df_xlmr[\"our_target\"].mean(), 1)\n",
    "# define new MultiIndex for the columns\n",
    "df_xlmr.columns = pd.MultiIndex.from_tuples(\n",
    "    [\n",
    "        (\"Kunz\", \"Target adapter\"),\n",
    "        (\"Kunz\", \"English adapter\"),\n",
    "        (\"Ours\", \"our_target\"),\n",
    "        (\"Ours\", \"our_baseline_en\"),\n",
    "        (\"Ours\", \"Approximation_method\"),\n",
    "    ],\n",
    "    names=[\"Source\", \"Method\"],\n",
    ")\n",
    "df_xlmr.rename(\n",
    "    columns={\"our_baseline_en\": \"English adapter\", \"our_target\": \"Target adapter\", \"Approximation_method\": \"TIPA\"},\n",
    "    level=\"Method\",\n",
    "    inplace=True,\n",
    ")\n",
    "# we rotate the dataframe\n",
    "df_xlmr = df_xlmr.T\n",
    "df_xlmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77113c75039f58be",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-02T16:00:19.556182600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0) (Optional) escape underscores in your XLM-R columns if present\n",
    "#    — only needed if any metric name contains '_' and you want it literal in LaTeX\n",
    "import numpy as np\n",
    "\n",
    "df_for_latex = df_xlmr.copy()\n",
    "\"\"\" We want row-wise stats instead!\n",
    "# 1) Compute per-column stats on df_for_latex \n",
    "col_stats = {}\n",
    "for col in df_for_latex.columns:\n",
    "    vals = df_for_latex[col].dropna().astype(float)\n",
    "    mn, md, mx = vals.min(), float(np.median(vals)), vals.max()\n",
    "    col_stats[col] = (mn, md, mx)\n",
    "\n",
    "# 2) Build your formatters dict using exactly the same MultiIndex column keys\n",
    "formatters = {}\n",
    "for col, (mn, md, mx) in col_stats.items():\n",
    "    # default-argument trick to bind mn, md, mx at definition time\n",
    "    fmt = lambda x, mn=mn, md=md, mx=mx: (\n",
    "        f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.3f}}}\"\n",
    "        if not pd.isna(x) else \"\"\n",
    "    )\n",
    "    formatters[col] = fmt\n",
    "# 3) Export to LaTeX\n",
    "latex_table = df_for_latex.to_latex(\n",
    "    escape=False,        # let \\tgrad[...] pass through\n",
    "    formatters=formatters,\n",
    "    multirow=True\n",
    ")\n",
    "\"\"\"\n",
    "# 1) Compute per-row (min, med, max) stats\n",
    "row_stats = {\n",
    "    idx: (row.min(skipna=True), float(row.median(skipna=True)), row.max(skipna=True))\n",
    "    for idx, row in df_for_latex.astype(float).iterrows()\n",
    "}\n",
    "\n",
    "# 2) Build a new DataFrame of formatted strings\n",
    "formatted = pd.DataFrame(index=df_for_latex.index, columns=df_for_latex.columns, dtype=object)\n",
    "\n",
    "for idx in df_for_latex.index:\n",
    "    mn, md, mx = row_stats[idx]\n",
    "    for col in df_for_latex.columns:\n",
    "        x = df_for_latex.at[idx, col]\n",
    "        if pd.isna(x):\n",
    "            formatted.at[idx, col] = \"\"\n",
    "        else:\n",
    "            formatted.at[idx, col] = f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.3f}}}\"\n",
    "# 3) Export the already-formatted table to LaTeX\n",
    "latex_table = formatted.to_latex(\n",
    "    escape=False,  # our macros must pass through\n",
    "    multirow=True,  # if you still want multirow on the first index level\n",
    ")\n",
    "\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4406482da2d907f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-02T16:00:19.558181900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_for_latex = df_xlmr.copy()\n",
    "\n",
    "# 1) Compute global stats over all numeric cells\n",
    "all_vals = pd.to_numeric(df_for_latex.values.ravel(), errors=\"coerce\")\n",
    "mn, md, mx = np.nanmin(all_vals), np.nanmedian(all_vals), np.nanmax(all_vals)\n",
    "\n",
    "\n",
    "# 2) Build one formatter that uses the global [mn, md, mx]\n",
    "def global_fmt(x, mn=mn, md=md, mx=mx):\n",
    "    return f\"\\\\tgrad[{mn:.1f}][{md:.1f}][{mx:.1f}]{{{x:.1f}}}\" if not pd.isna(x) else \"\"\n",
    "\n",
    "\n",
    "# assign the same formatter to every column\n",
    "formatters = {col: global_fmt for col in df_for_latex.columns}\n",
    "\n",
    "# 3) Export to LaTeX\n",
    "latex_table = df_for_latex.to_latex(\n",
    "    escape=False,  # let \\tgrad[...] pass through\n",
    "    formatters=formatters,\n",
    "    multirow=True,\n",
    ")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b673ecb01bd9c455",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-02T16:00:19.560267500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_for_latex = df_xlmr.copy()\n",
    "\n",
    "# 1) Compute global stats over all numeric cells\n",
    "all_vals = pd.to_numeric(df_for_latex.values.ravel(), errors=\"coerce\")\n",
    "mn, md, mx = np.nanmin(all_vals), np.nanmedian(all_vals), np.nanmax(all_vals)\n",
    "\n",
    "# 2) Precompute the max score in each column\n",
    "col_max = df_for_latex.max(axis=0)\n",
    "\n",
    "# 3) Build a new “string” DataFrame, applying gradient + bold on the column‐max\n",
    "df_str = df_for_latex.astype(object).copy()\n",
    "\n",
    "for idx, row in df_for_latex.iterrows():\n",
    "    for col in df_for_latex.columns:\n",
    "        x = row[col]\n",
    "        if pd.isna(x):\n",
    "            cell = \"\"\n",
    "        else:\n",
    "            # always wrap in the global gradient\n",
    "            grad = f\"\\\\tgrad[{mn:.1f}][{md:.1f}][{mx:.1f}]{{{x:.1f}}}\"\n",
    "            # bold if it’s the max in its column\n",
    "            if x == col_max[col]:\n",
    "                cell = f\"\\\\textbf{{{grad}}}\"\n",
    "            else:\n",
    "                cell = grad\n",
    "        df_str.at[idx, col] = cell\n",
    "\n",
    "# 4) Export to LaTeX\n",
    "latex_table = df_str.to_latex(\n",
    "    escape=False,  # allow \\tgrad and \\textbf through\n",
    "    multirow=True,\n",
    ")\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe722190aa9f867",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "SIB\n",
    "-> They make a table for XLM-R in function of family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4adf42fd06845afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:02:35.419445900Z",
     "start_time": "2025-06-02T16:02:35.414829900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indo-European is in dataframe\n",
      "Atlantic-Congo is in dataframe\n",
      "Afro-Asiatic is in dataframe\n",
      "Austronesian is in dataframe\n",
      "Turkic is in dataframe\n",
      "Sino-Tibetan is in dataframe\n",
      "Nilotic is in dataframe\n",
      "Dravidian is in dataframe\n",
      "Tai-Kadai is in dataframe\n",
      "Uralic is in dataframe\n",
      "Austroasiatic is in dataframe\n",
      "Mande is in dataframe\n",
      "Japonic is in dataframe\n",
      "Koreanic is in dataframe\n",
      "Mongolic-Khitan is in dataframe\n",
      "\t!!!Constructed not in dataframe\n",
      "\\In our system, Constructed is 'Artificial Language', so we can add it manually.\n",
      "Quechuan is in dataframe\n",
      "\t!!!Basque not in dataframe\n",
      "\tIn our system, Basque is 'unknown', so we can add it manually.\n",
      "Aymaran is in dataframe\n",
      "Tupian is in dataframe\n",
      "Kartvelian is in dataframe\n"
     ]
    }
   ],
   "source": [
    "# we load in the language family to the dataframe\n",
    "import pickle\n",
    "\n",
    "with open(\"../pickles/families.pkl\", \"rb\") as f:\n",
    "    families = pickle.load(f)\n",
    "# SIB scores, extracted from the paper (p230 aka 5)\n",
    "sib_og = {\n",
    "    \"Indo-European\": 82.4,\n",
    "    \"Atlantic-Congo\": 41.4,\n",
    "    \"Afro-Asiatic\": 67.4,\n",
    "    \"Austronesian\": 64.0,\n",
    "    \"Turkic\": 80.2,\n",
    "    \"Sino-Tibetan\": 57.9,\n",
    "    \"Nilotic\": 34.8,\n",
    "    \"Dravidian\": 87.8,\n",
    "    \"Tai-Kadai\": 68.4,\n",
    "    \"Uralic\": 89.1,\n",
    "    \"Austroasiatic\": 67.5,\n",
    "    \"Mande\": 32.5,\n",
    "    \"Japonic\": 89.3,\n",
    "    \"Koreanic\": 88.7,\n",
    "    \"Mongolic-Khitan\": 86.1,\n",
    "    \"Constructed\": 88.5,\n",
    "    \"Quechuan\": 46.3,\n",
    "    \"Basque\": 89.2,\n",
    "    \"Aymaran\": 39.1,\n",
    "    \"Tupian\": 61.3,\n",
    "    \"Kartvelian\": 89.1,\n",
    "}\n",
    "\n",
    "# we check if each of these families is present in our dataframe\n",
    "for family in sib_og.keys():\n",
    "    if family not in families.values():\n",
    "        print(f\"\\t!!!{family} not in dataframe\")\n",
    "        if family == \"Basque\":\n",
    "            print(\"\\tIn our system, Basque is 'unknown', so we can add it manually.\")\n",
    "        elif family == \"Constructed\":\n",
    "            print(\"\\In our system, Constructed is 'Artificial Language', so we can add it manually.\")\n",
    "    else:\n",
    "        print(f\"{family} is in dataframe\")\n",
    "# our later analysis show that Basque is Unknown in our calculation, but \"basque\" in SIB -> we fix this\n",
    "families[\"eu\"] = \"Basque\"\n",
    "families[\"eo\"] = \"Constructed\"  # Esperanto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "321475d77b29be3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:02:36.147154900Z",
     "start_time": "2025-06-02T16:02:36.125733700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TIPA</th>\n      <th>Closest Featural</th>\n      <th>No Train but Gain</th>\n      <th>XLM-R base</th>\n      <th>Language Family</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ace</th>\n      <td>42.647059</td>\n      <td>40.931373</td>\n      <td>42.156863</td>\n      <td>35.049020</td>\n      <td>Austronesian</td>\n    </tr>\n    <tr>\n      <th>acm</th>\n      <td>85.294118</td>\n      <td>74.019608</td>\n      <td>84.803922</td>\n      <td>83.823529</td>\n      <td>Afro-Asiatic</td>\n    </tr>\n    <tr>\n      <th>aeb</th>\n      <td>80.882353</td>\n      <td>53.921569</td>\n      <td>78.431373</td>\n      <td>80.392157</td>\n      <td>Afro-Asiatic</td>\n    </tr>\n    <tr>\n      <th>af</th>\n      <td>85.294118</td>\n      <td>83.823529</td>\n      <td>85.784314</td>\n      <td>87.254902</td>\n      <td>Indo-European</td>\n    </tr>\n    <tr>\n      <th>als</th>\n      <td>86.764706</td>\n      <td>82.352941</td>\n      <td>88.235294</td>\n      <td>87.745098</td>\n      <td>Indo-European</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>yo</th>\n      <td>25.980392</td>\n      <td>18.137255</td>\n      <td>21.078431</td>\n      <td>24.509804</td>\n      <td>Atlantic-Congo</td>\n    </tr>\n    <tr>\n      <th>yue</th>\n      <td>87.254902</td>\n      <td>86.764706</td>\n      <td>88.235294</td>\n      <td>87.254902</td>\n      <td>Sino-Tibetan</td>\n    </tr>\n    <tr>\n      <th>zh</th>\n      <td>88.235294</td>\n      <td>88.235294</td>\n      <td>87.990196</td>\n      <td>88.235294</td>\n      <td>Sino-Tibetan</td>\n    </tr>\n    <tr>\n      <th>zsm</th>\n      <td>90.686275</td>\n      <td>89.705882</td>\n      <td>88.725490</td>\n      <td>87.254902</td>\n      <td>Austronesian</td>\n    </tr>\n    <tr>\n      <th>zu</th>\n      <td>41.176471</td>\n      <td>32.352941</td>\n      <td>44.117647</td>\n      <td>40.686275</td>\n      <td>Atlantic-Congo</td>\n    </tr>\n  </tbody>\n</table>\n<p>175 rows × 5 columns</p>\n</div>",
      "text/plain": "          TIPA  Closest Featural  No Train but Gain  XLM-R base  \\\nace  42.647059         40.931373          42.156863   35.049020   \nacm  85.294118         74.019608          84.803922   83.823529   \naeb  80.882353         53.921569          78.431373   80.392157   \naf   85.294118         83.823529          85.784314   87.254902   \nals  86.764706         82.352941          88.235294   87.745098   \n..         ...               ...                ...         ...   \nyo   25.980392         18.137255          21.078431   24.509804   \nyue  87.254902         86.764706          88.235294   87.254902   \nzh   88.235294         88.235294          87.990196   88.235294   \nzsm  90.686275         89.705882          88.725490   87.254902   \nzu   41.176471         32.352941          44.117647   40.686275   \n\n    Language Family  \nace    Austronesian  \nacm    Afro-Asiatic  \naeb    Afro-Asiatic  \naf    Indo-European  \nals   Indo-European  \n..              ...  \nyo   Atlantic-Congo  \nyue    Sino-Tibetan  \nzh     Sino-Tibetan  \nzsm    Austronesian  \nzu   Atlantic-Congo  \n\n[175 rows x 5 columns]"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make a dataframe with sib scores for all languages\n",
    "sib = scores[\"sib\"]\n",
    "sib_df = pd.DataFrame.from_dict(sib, orient=\"index\")\n",
    "# we multiply all our values by 100\n",
    "sib_df = sib_df * 100\n",
    "\n",
    "# families is structured as {iso:family} so we can map it to the langs of the dataframe\n",
    "sib_df[\"Family\"] = sib_df.index.map(families)\n",
    "# we filter the categories: we know, from previous analysis, that \"reconstructed_syntactic\" is the best\n",
    "# we also keep the \"closest featural\", \"no train but gain\" and \"finetune\"\n",
    "sib_df = sib_df[[\"reconstructed_syntactic_limit\", \"baseline_closest_featural\", \"no_train_gain\", \"finetune\", \"Family\"]]\n",
    "rename_dict = {\n",
    "    \"reconstructed_syntactic_limit\": \"TIPA\",\n",
    "    \"baseline_closest_featural\": \"Closest Featural\",\n",
    "    \"no_train_gain\": \"No Train but Gain\",\n",
    "    \"finetune\": \"XLM-R base\",\n",
    "    \"Family\": \"Language Family\",\n",
    "}\n",
    "sib_df.rename(columns=rename_dict, inplace=True)\n",
    "# we drop languages that have \"NaN\" values -> 20-ish from 200-ish\n",
    "sib_df = sib_df.dropna(axis=0, how=\"any\")\n",
    "sib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "373aec6c302f77f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:02:37.858250Z",
     "start_time": "2025-06-02T16:02:37.837328700Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Language Family</th>\n      <th>TIPA</th>\n      <th>Closest Featural</th>\n      <th>No Train but Gain</th>\n      <th>XLM-R base</th>\n      <th>XLM-R large</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Afro-Asiatic</td>\n      <td>60.625721</td>\n      <td>50.259516</td>\n      <td>59.155133</td>\n      <td>58.621684</td>\n      <td>67.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Atlantic-Congo</td>\n      <td>32.978364</td>\n      <td>29.276538</td>\n      <td>31.135903</td>\n      <td>30.054091</td>\n      <td>41.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Austroasiatic</td>\n      <td>65.522876</td>\n      <td>65.032680</td>\n      <td>61.111111</td>\n      <td>66.503268</td>\n      <td>67.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Austronesian</td>\n      <td>62.312572</td>\n      <td>59.602076</td>\n      <td>61.317762</td>\n      <td>58.693772</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Aymaran</td>\n      <td>36.764706</td>\n      <td>41.666667</td>\n      <td>36.274510</td>\n      <td>21.078431</td>\n      <td>39.1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Basque</td>\n      <td>83.333333</td>\n      <td>80.882353</td>\n      <td>82.843137</td>\n      <td>80.392157</td>\n      <td>89.2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Constructed</td>\n      <td>83.333333</td>\n      <td>56.862745</td>\n      <td>82.352941</td>\n      <td>84.803922</td>\n      <td>88.5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Dravidian</td>\n      <td>82.475490</td>\n      <td>71.200980</td>\n      <td>81.004902</td>\n      <td>81.250000</td>\n      <td>87.8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Indo-European</td>\n      <td>77.625520</td>\n      <td>68.872549</td>\n      <td>75.902406</td>\n      <td>75.542187</td>\n      <td>82.4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Japonic</td>\n      <td>87.254902</td>\n      <td>88.235294</td>\n      <td>88.725490</td>\n      <td>87.745098</td>\n      <td>89.3</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Kartvelian</td>\n      <td>83.823529</td>\n      <td>77.941176</td>\n      <td>82.843137</td>\n      <td>76.960784</td>\n      <td>89.1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Koreanic</td>\n      <td>86.274510</td>\n      <td>64.705882</td>\n      <td>85.294118</td>\n      <td>83.823529</td>\n      <td>88.7</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Mande</td>\n      <td>26.715686</td>\n      <td>22.549020</td>\n      <td>26.225490</td>\n      <td>26.960784</td>\n      <td>32.5</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Mongolic-Khitan</td>\n      <td>81.372549</td>\n      <td>31.372549</td>\n      <td>76.470588</td>\n      <td>79.901961</td>\n      <td>86.1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Nilotic</td>\n      <td>24.509804</td>\n      <td>19.362745</td>\n      <td>22.549020</td>\n      <td>24.019608</td>\n      <td>34.8</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Quechuan</td>\n      <td>47.549020</td>\n      <td>57.352941</td>\n      <td>48.529412</td>\n      <td>37.745098</td>\n      <td>46.3</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Saharan</td>\n      <td>24.509804</td>\n      <td>27.450980</td>\n      <td>24.509804</td>\n      <td>24.019608</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Sino-Tibetan</td>\n      <td>48.713235</td>\n      <td>47.181373</td>\n      <td>46.783088</td>\n      <td>47.671569</td>\n      <td>57.9</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Tai-Kadai</td>\n      <td>64.542484</td>\n      <td>64.542484</td>\n      <td>61.764706</td>\n      <td>65.032680</td>\n      <td>68.4</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Tupian</td>\n      <td>62.254902</td>\n      <td>72.549020</td>\n      <td>56.372549</td>\n      <td>54.411765</td>\n      <td>61.3</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Turkic</td>\n      <td>72.994652</td>\n      <td>62.522282</td>\n      <td>69.295900</td>\n      <td>70.588235</td>\n      <td>80.2</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Uralic</td>\n      <td>87.254902</td>\n      <td>77.777778</td>\n      <td>85.457516</td>\n      <td>87.091503</td>\n      <td>89.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    Language Family       TIPA  Closest Featural  No Train but Gain  \\\n0      Afro-Asiatic  60.625721         50.259516          59.155133   \n1    Atlantic-Congo  32.978364         29.276538          31.135903   \n2     Austroasiatic  65.522876         65.032680          61.111111   \n3      Austronesian  62.312572         59.602076          61.317762   \n4           Aymaran  36.764706         41.666667          36.274510   \n5            Basque  83.333333         80.882353          82.843137   \n6       Constructed  83.333333         56.862745          82.352941   \n7         Dravidian  82.475490         71.200980          81.004902   \n8     Indo-European  77.625520         68.872549          75.902406   \n9           Japonic  87.254902         88.235294          88.725490   \n10       Kartvelian  83.823529         77.941176          82.843137   \n11         Koreanic  86.274510         64.705882          85.294118   \n12            Mande  26.715686         22.549020          26.225490   \n13  Mongolic-Khitan  81.372549         31.372549          76.470588   \n14          Nilotic  24.509804         19.362745          22.549020   \n15         Quechuan  47.549020         57.352941          48.529412   \n16          Saharan  24.509804         27.450980          24.509804   \n17     Sino-Tibetan  48.713235         47.181373          46.783088   \n18        Tai-Kadai  64.542484         64.542484          61.764706   \n19           Tupian  62.254902         72.549020          56.372549   \n20           Turkic  72.994652         62.522282          69.295900   \n21           Uralic  87.254902         77.777778          85.457516   \n\n    XLM-R base  XLM-R large  \n0    58.621684         67.4  \n1    30.054091         41.4  \n2    66.503268         67.5  \n3    58.693772         64.0  \n4    21.078431         39.1  \n5    80.392157         89.2  \n6    84.803922         88.5  \n7    81.250000         87.8  \n8    75.542187         82.4  \n9    87.745098         89.3  \n10   76.960784         89.1  \n11   83.823529         88.7  \n12   26.960784         32.5  \n13   79.901961         86.1  \n14   24.019608         34.8  \n15   37.745098         46.3  \n16   24.019608          NaN  \n17   47.671569         57.9  \n18   65.032680         68.4  \n19   54.411765         61.3  \n20   70.588235         80.2  \n21   87.091503         89.1  "
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make a dataframe \"sib_averages\" in which we average the scores for each family\n",
    "sib_averages = sib_df.groupby(\"Language Family\").mean().reset_index()\n",
    "# we add the original SIB scores to the dataframe\n",
    "sib_averages[\"XLM-R large\"] = sib_averages[\"Language Family\"].map(sib_og)\n",
    "# we add a column \"average\" for each method, loaded in from the \"sib_average\" dict\n",
    "sib_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f404f8defd3bf5b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:02:39.232031600Z",
     "start_time": "2025-06-02T16:02:39.211824500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"3\" halign=\"left\">Adapter‐based methods</th>\n      <th colspan=\"2\" halign=\"left\">Fine‐tuning methods</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>TIPA</th>\n      <th>Closest Featural</th>\n      <th>No Train but Gain</th>\n      <th>XLM‐R base</th>\n      <th>XLM‐R large</th>\n    </tr>\n    <tr>\n      <th>Language Family</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Afro-Asiatic</th>\n      <td>60.625721</td>\n      <td>50.259516</td>\n      <td>59.155133</td>\n      <td>58.621684</td>\n      <td>67.4</td>\n    </tr>\n    <tr>\n      <th>Atlantic-Congo</th>\n      <td>32.978364</td>\n      <td>29.276538</td>\n      <td>31.135903</td>\n      <td>30.054091</td>\n      <td>41.4</td>\n    </tr>\n    <tr>\n      <th>Austroasiatic</th>\n      <td>65.522876</td>\n      <td>65.032680</td>\n      <td>61.111111</td>\n      <td>66.503268</td>\n      <td>67.5</td>\n    </tr>\n    <tr>\n      <th>Austronesian</th>\n      <td>62.312572</td>\n      <td>59.602076</td>\n      <td>61.317762</td>\n      <td>58.693772</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>Aymaran</th>\n      <td>36.764706</td>\n      <td>41.666667</td>\n      <td>36.274510</td>\n      <td>21.078431</td>\n      <td>39.1</td>\n    </tr>\n    <tr>\n      <th>Basque</th>\n      <td>83.333333</td>\n      <td>80.882353</td>\n      <td>82.843137</td>\n      <td>80.392157</td>\n      <td>89.2</td>\n    </tr>\n    <tr>\n      <th>Constructed</th>\n      <td>83.333333</td>\n      <td>56.862745</td>\n      <td>82.352941</td>\n      <td>84.803922</td>\n      <td>88.5</td>\n    </tr>\n    <tr>\n      <th>Dravidian</th>\n      <td>82.475490</td>\n      <td>71.200980</td>\n      <td>81.004902</td>\n      <td>81.250000</td>\n      <td>87.8</td>\n    </tr>\n    <tr>\n      <th>Indo-European</th>\n      <td>77.625520</td>\n      <td>68.872549</td>\n      <td>75.902406</td>\n      <td>75.542187</td>\n      <td>82.4</td>\n    </tr>\n    <tr>\n      <th>Japonic</th>\n      <td>87.254902</td>\n      <td>88.235294</td>\n      <td>88.725490</td>\n      <td>87.745098</td>\n      <td>89.3</td>\n    </tr>\n    <tr>\n      <th>Kartvelian</th>\n      <td>83.823529</td>\n      <td>77.941176</td>\n      <td>82.843137</td>\n      <td>76.960784</td>\n      <td>89.1</td>\n    </tr>\n    <tr>\n      <th>Koreanic</th>\n      <td>86.274510</td>\n      <td>64.705882</td>\n      <td>85.294118</td>\n      <td>83.823529</td>\n      <td>88.7</td>\n    </tr>\n    <tr>\n      <th>Mande</th>\n      <td>26.715686</td>\n      <td>22.549020</td>\n      <td>26.225490</td>\n      <td>26.960784</td>\n      <td>32.5</td>\n    </tr>\n    <tr>\n      <th>Mongolic-Khitan</th>\n      <td>81.372549</td>\n      <td>31.372549</td>\n      <td>76.470588</td>\n      <td>79.901961</td>\n      <td>86.1</td>\n    </tr>\n    <tr>\n      <th>Nilotic</th>\n      <td>24.509804</td>\n      <td>19.362745</td>\n      <td>22.549020</td>\n      <td>24.019608</td>\n      <td>34.8</td>\n    </tr>\n    <tr>\n      <th>Quechuan</th>\n      <td>47.549020</td>\n      <td>57.352941</td>\n      <td>48.529412</td>\n      <td>37.745098</td>\n      <td>46.3</td>\n    </tr>\n    <tr>\n      <th>Sino-Tibetan</th>\n      <td>48.713235</td>\n      <td>47.181373</td>\n      <td>46.783088</td>\n      <td>47.671569</td>\n      <td>57.9</td>\n    </tr>\n    <tr>\n      <th>Tai-Kadai</th>\n      <td>64.542484</td>\n      <td>64.542484</td>\n      <td>61.764706</td>\n      <td>65.032680</td>\n      <td>68.4</td>\n    </tr>\n    <tr>\n      <th>Tupian</th>\n      <td>62.254902</td>\n      <td>72.549020</td>\n      <td>56.372549</td>\n      <td>54.411765</td>\n      <td>61.3</td>\n    </tr>\n    <tr>\n      <th>Turkic</th>\n      <td>72.994652</td>\n      <td>62.522282</td>\n      <td>69.295900</td>\n      <td>70.588235</td>\n      <td>80.2</td>\n    </tr>\n    <tr>\n      <th>Uralic</th>\n      <td>87.254902</td>\n      <td>77.777778</td>\n      <td>85.457516</td>\n      <td>87.091503</td>\n      <td>89.1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                Adapter‐based methods                                     \\\n                                 TIPA Closest Featural No Train but Gain   \nLanguage Family                                                            \nAfro-Asiatic                60.625721        50.259516         59.155133   \nAtlantic-Congo              32.978364        29.276538         31.135903   \nAustroasiatic               65.522876        65.032680         61.111111   \nAustronesian                62.312572        59.602076         61.317762   \nAymaran                     36.764706        41.666667         36.274510   \nBasque                      83.333333        80.882353         82.843137   \nConstructed                 83.333333        56.862745         82.352941   \nDravidian                   82.475490        71.200980         81.004902   \nIndo-European               77.625520        68.872549         75.902406   \nJaponic                     87.254902        88.235294         88.725490   \nKartvelian                  83.823529        77.941176         82.843137   \nKoreanic                    86.274510        64.705882         85.294118   \nMande                       26.715686        22.549020         26.225490   \nMongolic-Khitan             81.372549        31.372549         76.470588   \nNilotic                     24.509804        19.362745         22.549020   \nQuechuan                    47.549020        57.352941         48.529412   \nSino-Tibetan                48.713235        47.181373         46.783088   \nTai-Kadai                   64.542484        64.542484         61.764706   \nTupian                      62.254902        72.549020         56.372549   \nTurkic                      72.994652        62.522282         69.295900   \nUralic                      87.254902        77.777778         85.457516   \n\n                Fine‐tuning methods              \n                         XLM‐R base XLM‐R large  \nLanguage Family                                  \nAfro-Asiatic              58.621684        67.4  \nAtlantic-Congo            30.054091        41.4  \nAustroasiatic             66.503268        67.5  \nAustronesian              58.693772        64.0  \nAymaran                   21.078431        39.1  \nBasque                    80.392157        89.2  \nConstructed               84.803922        88.5  \nDravidian                 81.250000        87.8  \nIndo-European             75.542187        82.4  \nJaponic                   87.745098        89.3  \nKartvelian                76.960784        89.1  \nKoreanic                  83.823529        88.7  \nMande                     26.960784        32.5  \nMongolic-Khitan           79.901961        86.1  \nNilotic                   24.019608        34.8  \nQuechuan                  37.745098        46.3  \nSino-Tibetan              47.671569        57.9  \nTai-Kadai                 65.032680        68.4  \nTupian                    54.411765        61.3  \nTurkic                    70.588235        80.2  \nUralic                    87.091503        89.1  "
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we drop languages with NaN again\n",
    "sib_averages = sib_averages.dropna(axis=0, how=\"any\")\n",
    "sib_averages = sib_averages.set_index(\"Language Family\")\n",
    "\n",
    "new_columns = [\n",
    "    (\"Adapter‐based methods\", \"TIPA\"),\n",
    "    (\"Adapter‐based methods\", \"Closest Featural\"),\n",
    "    (\"Adapter‐based methods\", \"No Train but Gain\"),\n",
    "    (\"Fine‐tuning methods\", \"XLM‐R base\"),\n",
    "    (\"Fine‐tuning methods\", \"XLM‐R large\"),\n",
    "]\n",
    "sib_averages.columns = pd.MultiIndex.from_tuples(new_columns)\n",
    "\n",
    "sib_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf640410a264727c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:02:40.713703400Z",
     "start_time": "2025-06-02T16:02:40.706292500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      " & \\multicolumn{3}{r}{Adapter‐based methods} & \\multicolumn{2}{r}{Fine‐tuning methods} \\\\\n",
      " & TIPA & Closest Featural & No Train but Gain & XLM‐R base & XLM‐R large \\\\\n",
      "Language Family &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "Afro-Asiatic & 60.6 & 50.3 & 59.2 & 58.6 & 67.4 \\\\\n",
      "Atlantic-Congo & 33.0 & 29.3 & 31.1 & 30.1 & 41.4 \\\\\n",
      "Austroasiatic & 65.5 & 65.0 & 61.1 & 66.5 & 67.5 \\\\\n",
      "Austronesian & 62.3 & 59.6 & 61.3 & 58.7 & 64.0 \\\\\n",
      "Aymaran & 36.8 & 41.7 & 36.3 & 21.1 & 39.1 \\\\\n",
      "Basque & 83.3 & 80.9 & 82.8 & 80.4 & 89.2 \\\\\n",
      "Constructed & 83.3 & 56.9 & 82.4 & 84.8 & 88.5 \\\\\n",
      "Dravidian & 82.5 & 71.2 & 81.0 & 81.2 & 87.8 \\\\\n",
      "Indo-European & 77.6 & 68.9 & 75.9 & 75.5 & 82.4 \\\\\n",
      "Japonic & 87.3 & 88.2 & 88.7 & 87.7 & 89.3 \\\\\n",
      "Kartvelian & 83.8 & 77.9 & 82.8 & 77.0 & 89.1 \\\\\n",
      "Koreanic & 86.3 & 64.7 & 85.3 & 83.8 & 88.7 \\\\\n",
      "Mande & 26.7 & 22.5 & 26.2 & 27.0 & 32.5 \\\\\n",
      "Mongolic-Khitan & 81.4 & 31.4 & 76.5 & 79.9 & 86.1 \\\\\n",
      "Nilotic & 24.5 & 19.4 & 22.5 & 24.0 & 34.8 \\\\\n",
      "Quechuan & 47.5 & 57.4 & 48.5 & 37.7 & 46.3 \\\\\n",
      "Sino-Tibetan & 48.7 & 47.2 & 46.8 & 47.7 & 57.9 \\\\\n",
      "Tai-Kadai & 64.5 & 64.5 & 61.8 & 65.0 & 68.4 \\\\\n",
      "Tupian & 62.3 & 72.5 & 56.4 & 54.4 & 61.3 \\\\\n",
      "Turkic & 73.0 & 62.5 & 69.3 & 70.6 & 80.2 \\\\\n",
      "Uralic & 87.3 & 77.8 & 85.5 & 87.1 & 89.1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# we export the dataframe to latex\n",
    "latex_sib = sib_averages.to_latex(escape=False, multirow=True, float_format=\"%.1f\")\n",
    "print(latex_sib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89de239738240326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:02:41.738471700Z",
     "start_time": "2025-06-02T16:02:41.731559800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      " & \\multicolumn{3}{r}{Adapter‐based methods} & \\multicolumn{2}{r}{Fine‐tuning methods} \\\\\n",
      " & TIPA & Closest Featural & No Train but Gain & XLM‐R base & XLM‐R large \\\\\n",
      "Language Family &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "Afro-Asiatic & \\tgrad[19.363][65.033][89.300]{60.6} & \\tgrad[19.363][65.033][89.300]{50.3} & \\tgrad[19.363][65.033][89.300]{59.2} & \\tgrad[19.363][65.033][89.300]{58.6} & \\textbf{\\tgrad[19.363][65.033][89.300]{67.4}} \\\\\n",
      "Atlantic-Congo & \\tgrad[19.363][65.033][89.300]{33.0} & \\tgrad[19.363][65.033][89.300]{29.3} & \\tgrad[19.363][65.033][89.300]{31.1} & \\tgrad[19.363][65.033][89.300]{30.1} & \\textbf{\\tgrad[19.363][65.033][89.300]{41.4}} \\\\\n",
      "Austroasiatic & \\tgrad[19.363][65.033][89.300]{65.5} & \\tgrad[19.363][65.033][89.300]{65.0} & \\tgrad[19.363][65.033][89.300]{61.1} & \\tgrad[19.363][65.033][89.300]{66.5} & \\textbf{\\tgrad[19.363][65.033][89.300]{67.5}} \\\\\n",
      "Austronesian & \\tgrad[19.363][65.033][89.300]{62.3} & \\tgrad[19.363][65.033][89.300]{59.6} & \\tgrad[19.363][65.033][89.300]{61.3} & \\tgrad[19.363][65.033][89.300]{58.7} & \\textbf{\\tgrad[19.363][65.033][89.300]{64.0}} \\\\\n",
      "Aymaran & \\tgrad[19.363][65.033][89.300]{36.8} & \\textbf{\\tgrad[19.363][65.033][89.300]{41.7}} & \\tgrad[19.363][65.033][89.300]{36.3} & \\tgrad[19.363][65.033][89.300]{21.1} & \\tgrad[19.363][65.033][89.300]{39.1} \\\\\n",
      "Basque & \\tgrad[19.363][65.033][89.300]{83.3} & \\tgrad[19.363][65.033][89.300]{80.9} & \\tgrad[19.363][65.033][89.300]{82.8} & \\tgrad[19.363][65.033][89.300]{80.4} & \\textbf{\\tgrad[19.363][65.033][89.300]{89.2}} \\\\\n",
      "Constructed & \\tgrad[19.363][65.033][89.300]{83.3} & \\tgrad[19.363][65.033][89.300]{56.9} & \\tgrad[19.363][65.033][89.300]{82.4} & \\tgrad[19.363][65.033][89.300]{84.8} & \\textbf{\\tgrad[19.363][65.033][89.300]{88.5}} \\\\\n",
      "Dravidian & \\tgrad[19.363][65.033][89.300]{82.5} & \\tgrad[19.363][65.033][89.300]{71.2} & \\tgrad[19.363][65.033][89.300]{81.0} & \\tgrad[19.363][65.033][89.300]{81.2} & \\textbf{\\tgrad[19.363][65.033][89.300]{87.8}} \\\\\n",
      "Indo-European & \\tgrad[19.363][65.033][89.300]{77.6} & \\tgrad[19.363][65.033][89.300]{68.9} & \\tgrad[19.363][65.033][89.300]{75.9} & \\tgrad[19.363][65.033][89.300]{75.5} & \\textbf{\\tgrad[19.363][65.033][89.300]{82.4}} \\\\\n",
      "Japonic & \\tgrad[19.363][65.033][89.300]{87.3} & \\tgrad[19.363][65.033][89.300]{88.2} & \\tgrad[19.363][65.033][89.300]{88.7} & \\tgrad[19.363][65.033][89.300]{87.7} & \\textbf{\\tgrad[19.363][65.033][89.300]{89.3}} \\\\\n",
      "Kartvelian & \\tgrad[19.363][65.033][89.300]{83.8} & \\tgrad[19.363][65.033][89.300]{77.9} & \\tgrad[19.363][65.033][89.300]{82.8} & \\tgrad[19.363][65.033][89.300]{77.0} & \\textbf{\\tgrad[19.363][65.033][89.300]{89.1}} \\\\\n",
      "Koreanic & \\tgrad[19.363][65.033][89.300]{86.3} & \\tgrad[19.363][65.033][89.300]{64.7} & \\tgrad[19.363][65.033][89.300]{85.3} & \\tgrad[19.363][65.033][89.300]{83.8} & \\textbf{\\tgrad[19.363][65.033][89.300]{88.7}} \\\\\n",
      "Mande & \\tgrad[19.363][65.033][89.300]{26.7} & \\tgrad[19.363][65.033][89.300]{22.5} & \\tgrad[19.363][65.033][89.300]{26.2} & \\tgrad[19.363][65.033][89.300]{27.0} & \\textbf{\\tgrad[19.363][65.033][89.300]{32.5}} \\\\\n",
      "Mongolic-Khitan & \\tgrad[19.363][65.033][89.300]{81.4} & \\tgrad[19.363][65.033][89.300]{31.4} & \\tgrad[19.363][65.033][89.300]{76.5} & \\tgrad[19.363][65.033][89.300]{79.9} & \\textbf{\\tgrad[19.363][65.033][89.300]{86.1}} \\\\\n",
      "Nilotic & \\tgrad[19.363][65.033][89.300]{24.5} & \\tgrad[19.363][65.033][89.300]{19.4} & \\tgrad[19.363][65.033][89.300]{22.5} & \\tgrad[19.363][65.033][89.300]{24.0} & \\textbf{\\tgrad[19.363][65.033][89.300]{34.8}} \\\\\n",
      "Quechuan & \\tgrad[19.363][65.033][89.300]{47.5} & \\textbf{\\tgrad[19.363][65.033][89.300]{57.4}} & \\tgrad[19.363][65.033][89.300]{48.5} & \\tgrad[19.363][65.033][89.300]{37.7} & \\tgrad[19.363][65.033][89.300]{46.3} \\\\\n",
      "Sino-Tibetan & \\tgrad[19.363][65.033][89.300]{48.7} & \\tgrad[19.363][65.033][89.300]{47.2} & \\tgrad[19.363][65.033][89.300]{46.8} & \\tgrad[19.363][65.033][89.300]{47.7} & \\textbf{\\tgrad[19.363][65.033][89.300]{57.9}} \\\\\n",
      "Tai-Kadai & \\tgrad[19.363][65.033][89.300]{64.5} & \\tgrad[19.363][65.033][89.300]{64.5} & \\tgrad[19.363][65.033][89.300]{61.8} & \\tgrad[19.363][65.033][89.300]{65.0} & \\textbf{\\tgrad[19.363][65.033][89.300]{68.4}} \\\\\n",
      "Tupian & \\tgrad[19.363][65.033][89.300]{62.3} & \\textbf{\\tgrad[19.363][65.033][89.300]{72.5}} & \\tgrad[19.363][65.033][89.300]{56.4} & \\tgrad[19.363][65.033][89.300]{54.4} & \\tgrad[19.363][65.033][89.300]{61.3} \\\\\n",
      "Turkic & \\tgrad[19.363][65.033][89.300]{73.0} & \\tgrad[19.363][65.033][89.300]{62.5} & \\tgrad[19.363][65.033][89.300]{69.3} & \\tgrad[19.363][65.033][89.300]{70.6} & \\textbf{\\tgrad[19.363][65.033][89.300]{80.2}} \\\\\n",
      "Uralic & \\tgrad[19.363][65.033][89.300]{87.3} & \\tgrad[19.363][65.033][89.300]{77.8} & \\tgrad[19.363][65.033][89.300]{85.5} & \\tgrad[19.363][65.033][89.300]{87.1} & \\textbf{\\tgrad[19.363][65.033][89.300]{89.1}} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "# 1) compute global [min, median, max] over every numeric cell\n",
    "all_vals = pd.to_numeric(sib_averages.values.ravel(), errors=\"coerce\")\n",
    "mn, md, mx = np.nanmin(all_vals), float(np.nanmedian(all_vals)), np.nanmax(all_vals)\n",
    "\n",
    "# 2) build a string‐typed DataFrame, applying global gradient + bold on row‐max\n",
    "sib_str = sib_averages.astype(object).copy()\n",
    "\n",
    "for idx, row in sib_averages.iterrows():\n",
    "    # compute the maximum value in this row (ignoring NaNs)\n",
    "    row_max = row.max()\n",
    "    for col in sib_averages.columns:\n",
    "        x = row[col]\n",
    "        if pd.isna(x):\n",
    "            cell = \"\"\n",
    "        else:\n",
    "            # global gradient\n",
    "            grad = f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.1f}}}\"\n",
    "            # bold if it’s the max in its row\n",
    "            cell = f\"\\\\textbf{{{grad}}}\" if x == row_max else grad\n",
    "        sib_str.at[idx, col] = cell\n",
    "\n",
    "# 3) export to LaTeX (letting \\tgrad and \\textbf pass through)\n",
    "latex = sib_str.to_latex(\n",
    "    escape=False,\n",
    "    multirow=True,  # keep multirow on the index if you like\n",
    ")\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b973393a07916104",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Or, only bolding adapter-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4bf3f24eff75e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T16:02:43.400999900Z",
     "start_time": "2025-06-02T16:02:43.375096Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      " & \\multicolumn{3}{r}{Adapter‐based methods} & \\multicolumn{2}{r}{Fine‐tuning methods} \\\\\n",
      " & TIPA & Closest Featural & No Train but Gain & XLM‐R base & XLM‐R large \\\\\n",
      "Language Family &  &  &  &  &  \\\\\n",
      "\\midrule\n",
      "Afro-Asiatic & \\textbf{\\tgrad[19.363][65.033][89.300]{60.6}} & \\tgrad[19.363][65.033][89.300]{50.3} & \\tgrad[19.363][65.033][89.300]{59.2} & \\tgrad[19.363][65.033][89.300]{58.6} & \\textbf{\\tgrad[19.363][65.033][89.300]{67.4}} \\\\\n",
      "Atlantic-Congo & \\textbf{\\tgrad[19.363][65.033][89.300]{33.0}} & \\tgrad[19.363][65.033][89.300]{29.3} & \\tgrad[19.363][65.033][89.300]{31.1} & \\tgrad[19.363][65.033][89.300]{30.1} & \\textbf{\\tgrad[19.363][65.033][89.300]{41.4}} \\\\\n",
      "Austroasiatic & \\textbf{\\tgrad[19.363][65.033][89.300]{65.5}} & \\tgrad[19.363][65.033][89.300]{65.0} & \\tgrad[19.363][65.033][89.300]{61.1} & \\tgrad[19.363][65.033][89.300]{66.5} & \\textbf{\\tgrad[19.363][65.033][89.300]{67.5}} \\\\\n",
      "Austronesian & \\textbf{\\tgrad[19.363][65.033][89.300]{62.3}} & \\tgrad[19.363][65.033][89.300]{59.6} & \\tgrad[19.363][65.033][89.300]{61.3} & \\tgrad[19.363][65.033][89.300]{58.7} & \\textbf{\\tgrad[19.363][65.033][89.300]{64.0}} \\\\\n",
      "Aymaran & \\tgrad[19.363][65.033][89.300]{36.8} & \\textbf{\\tgrad[19.363][65.033][89.300]{41.7}} & \\tgrad[19.363][65.033][89.300]{36.3} & \\tgrad[19.363][65.033][89.300]{21.1} & \\textbf{\\tgrad[19.363][65.033][89.300]{39.1}} \\\\\n",
      "Basque & \\textbf{\\tgrad[19.363][65.033][89.300]{83.3}} & \\tgrad[19.363][65.033][89.300]{80.9} & \\tgrad[19.363][65.033][89.300]{82.8} & \\tgrad[19.363][65.033][89.300]{80.4} & \\textbf{\\tgrad[19.363][65.033][89.300]{89.2}} \\\\\n",
      "Constructed & \\textbf{\\tgrad[19.363][65.033][89.300]{83.3}} & \\tgrad[19.363][65.033][89.300]{56.9} & \\tgrad[19.363][65.033][89.300]{82.4} & \\tgrad[19.363][65.033][89.300]{84.8} & \\textbf{\\tgrad[19.363][65.033][89.300]{88.5}} \\\\\n",
      "Dravidian & \\textbf{\\tgrad[19.363][65.033][89.300]{82.5}} & \\tgrad[19.363][65.033][89.300]{71.2} & \\tgrad[19.363][65.033][89.300]{81.0} & \\tgrad[19.363][65.033][89.300]{81.2} & \\textbf{\\tgrad[19.363][65.033][89.300]{87.8}} \\\\\n",
      "Indo-European & \\textbf{\\tgrad[19.363][65.033][89.300]{77.6}} & \\tgrad[19.363][65.033][89.300]{68.9} & \\tgrad[19.363][65.033][89.300]{75.9} & \\tgrad[19.363][65.033][89.300]{75.5} & \\textbf{\\tgrad[19.363][65.033][89.300]{82.4}} \\\\\n",
      "Japonic & \\tgrad[19.363][65.033][89.300]{87.3} & \\tgrad[19.363][65.033][89.300]{88.2} & \\textbf{\\tgrad[19.363][65.033][89.300]{88.7}} & \\tgrad[19.363][65.033][89.300]{87.7} & \\textbf{\\tgrad[19.363][65.033][89.300]{89.3}} \\\\\n",
      "Kartvelian & \\textbf{\\tgrad[19.363][65.033][89.300]{83.8}} & \\tgrad[19.363][65.033][89.300]{77.9} & \\tgrad[19.363][65.033][89.300]{82.8} & \\tgrad[19.363][65.033][89.300]{77.0} & \\textbf{\\tgrad[19.363][65.033][89.300]{89.1}} \\\\\n",
      "Koreanic & \\textbf{\\tgrad[19.363][65.033][89.300]{86.3}} & \\tgrad[19.363][65.033][89.300]{64.7} & \\tgrad[19.363][65.033][89.300]{85.3} & \\tgrad[19.363][65.033][89.300]{83.8} & \\textbf{\\tgrad[19.363][65.033][89.300]{88.7}} \\\\\n",
      "Mande & \\textbf{\\tgrad[19.363][65.033][89.300]{26.7}} & \\tgrad[19.363][65.033][89.300]{22.5} & \\tgrad[19.363][65.033][89.300]{26.2} & \\tgrad[19.363][65.033][89.300]{27.0} & \\textbf{\\tgrad[19.363][65.033][89.300]{32.5}} \\\\\n",
      "Mongolic-Khitan & \\textbf{\\tgrad[19.363][65.033][89.300]{81.4}} & \\tgrad[19.363][65.033][89.300]{31.4} & \\tgrad[19.363][65.033][89.300]{76.5} & \\tgrad[19.363][65.033][89.300]{79.9} & \\textbf{\\tgrad[19.363][65.033][89.300]{86.1}} \\\\\n",
      "Nilotic & \\textbf{\\tgrad[19.363][65.033][89.300]{24.5}} & \\tgrad[19.363][65.033][89.300]{19.4} & \\tgrad[19.363][65.033][89.300]{22.5} & \\tgrad[19.363][65.033][89.300]{24.0} & \\textbf{\\tgrad[19.363][65.033][89.300]{34.8}} \\\\\n",
      "Quechuan & \\tgrad[19.363][65.033][89.300]{47.5} & \\textbf{\\tgrad[19.363][65.033][89.300]{57.4}} & \\tgrad[19.363][65.033][89.300]{48.5} & \\tgrad[19.363][65.033][89.300]{37.7} & \\textbf{\\tgrad[19.363][65.033][89.300]{46.3}} \\\\\n",
      "Sino-Tibetan & \\textbf{\\tgrad[19.363][65.033][89.300]{48.7}} & \\tgrad[19.363][65.033][89.300]{47.2} & \\tgrad[19.363][65.033][89.300]{46.8} & \\tgrad[19.363][65.033][89.300]{47.7} & \\textbf{\\tgrad[19.363][65.033][89.300]{57.9}} \\\\\n",
      "Tai-Kadai & \\textbf{\\tgrad[19.363][65.033][89.300]{64.5}} & \\textbf{\\tgrad[19.363][65.033][89.300]{64.5}} & \\tgrad[19.363][65.033][89.300]{61.8} & \\tgrad[19.363][65.033][89.300]{65.0} & \\textbf{\\tgrad[19.363][65.033][89.300]{68.4}} \\\\\n",
      "Tupian & \\tgrad[19.363][65.033][89.300]{62.3} & \\textbf{\\tgrad[19.363][65.033][89.300]{72.5}} & \\tgrad[19.363][65.033][89.300]{56.4} & \\tgrad[19.363][65.033][89.300]{54.4} & \\textbf{\\tgrad[19.363][65.033][89.300]{61.3}} \\\\\n",
      "Turkic & \\textbf{\\tgrad[19.363][65.033][89.300]{73.0}} & \\tgrad[19.363][65.033][89.300]{62.5} & \\tgrad[19.363][65.033][89.300]{69.3} & \\tgrad[19.363][65.033][89.300]{70.6} & \\textbf{\\tgrad[19.363][65.033][89.300]{80.2}} \\\\\n",
      "Uralic & \\textbf{\\tgrad[19.363][65.033][89.300]{87.3}} & \\tgrad[19.363][65.033][89.300]{77.8} & \\tgrad[19.363][65.033][89.300]{85.5} & \\tgrad[19.363][65.033][89.300]{87.1} & \\textbf{\\tgrad[19.363][65.033][89.300]{89.1}} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) compute global [min, median, max] over every numeric cell\n",
    "all_vals = pd.to_numeric(sib_averages.values.ravel(), errors=\"coerce\")\n",
    "mn, md, mx = np.nanmin(all_vals), float(np.nanmedian(all_vals)), np.nanmax(all_vals)\n",
    "\n",
    "# 2) build a string‐typed DataFrame, applying global gradient + bold on adapter‐based row‐max\n",
    "sib_str = sib_averages.astype(object).copy()\n",
    "\n",
    "for idx, row in sib_averages.iterrows():\n",
    "    # compute the maximum over only the \"Adapter-based methods\" columns\n",
    "    adapter_row = row[\"Adapter‐based methods\"]\n",
    "    finetune_row = row[\"Fine‐tuning methods\"]\n",
    "    row_max_adapter = adapter_row.max()\n",
    "    row_max_finetune = finetune_row.max()\n",
    "    for col in sib_averages.columns:\n",
    "        x = row[col]\n",
    "        if pd.isna(x):\n",
    "            cell = \"\"\n",
    "        else:\n",
    "            # global gradient\n",
    "            grad = f\"\\\\tgrad[{mn:.3f}][{md:.3f}][{mx:.3f}]{{{x:.1f}}}\"\n",
    "            # bold if it’s under \"Adapter-based methods\" and equals that row’s adapter‐max\n",
    "            if col[0] == \"Adapter‐based methods\" and x == row_max_adapter:\n",
    "                cell = f\"\\\\textbf{{{grad}}}\"\n",
    "            elif col[0] == \"Fine‐tuning methods\" and x == row_max_finetune:\n",
    "                cell = f\"\\\\textbf{{{grad}}}\"\n",
    "            else:\n",
    "                cell = grad\n",
    "        sib_str.at[idx, col] = cell\n",
    "\n",
    "# 3) export to LaTeX (letting \\tgrad and \\textbf pass through)\n",
    "latex = sib_str.to_latex(escape=False, multirow=True)\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bad976ce2cc88229",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T17:50:51.207794600Z",
     "start_time": "2025-06-02T17:50:51.205787600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aymaran:\n",
      "\tayr -> 36.8\n",
      "\t\tNo adapter for ayr\n",
      "\n",
      "Japonic:\n",
      "\tja -> 87.3\n",
      "\t\tAdapter already exists for ja\n",
      "\n",
      "Tupian:\n",
      "\tgn -> 62.3\n",
      "\t\tAdapter already exists for gn\n",
      "\n",
      "Quechuan:\n",
      "\tquy -> 47.5\n",
      "\t\tAdapter exists for the macro 'qu' which is Quechua!\n",
      "\t\tNo adapter for quy\n"
     ]
    }
   ],
   "source": [
    "existing_adapters = [\n",
    "    \"th\",\n",
    "    \"my\",\n",
    "    \"hi\",\n",
    "    \"ilo\",\n",
    "    \"ht\",\n",
    "    \"tr\",\n",
    "    \"mi\",\n",
    "    \"vi\",\n",
    "    \"is\",\n",
    "    \"it\",\n",
    "    \"ta\",\n",
    "    \"jv\",\n",
    "    \"ja\",\n",
    "    \"sw\",\n",
    "    \"qu\",\n",
    "    \"de\",\n",
    "    \"el\",\n",
    "    \"et\",\n",
    "    \"ru\",\n",
    "    \"gn\",\n",
    "    \"id\",\n",
    "    \"en\",\n",
    "    \"ar\",\n",
    "    \"es\",\n",
    "    \"tk\",\n",
    "    \"zh\",\n",
    "    \"mhr\",\n",
    "    \"cdo\",\n",
    "    \"xmf\",\n",
    "    \"eu\",\n",
    "    \"sr\",\n",
    "]\n",
    "# we print the languages in the following families\n",
    "families_of_interest = [\"Aymaran\", \"Japonic\", \"Tupian\", \"Quechuan\"]\n",
    "for family in families_of_interest:\n",
    "    print(f\"\\n{family}:\")\n",
    "    langs = sib_df[sib_df[\"Language Family\"] == family].index.tolist()\n",
    "    for lang in langs:\n",
    "        print(f\"\\t{lang} -> {sib_df.loc[lang, 'TIPA']:.1f}\")\n",
    "        if lang in existing_adapters:\n",
    "            print(f\"\\t\\tAdapter already exists for {lang}\")\n",
    "        else:\n",
    "            if lang == \"quy\":\n",
    "                print(\"\\t\\tAdapter exists for the macro 'qu' which is Quechua!\")\n",
    "            print(f\"\\t\\tNo adapter for {lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d72a4e4d6914c8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
