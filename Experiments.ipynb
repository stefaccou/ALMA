{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8bc84e9d77696c8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Experiments\n",
    "We check where typological grouping can be most effective,\n",
    "and which type of typological grouping works best.\n",
    "\n",
    "1. Grouping in function aggregation\n",
    "- Phylogeny OR typology inspired stacks\n",
    "- Greater dataset for adapter training\n",
    "2. Parameter aggregation\n",
    "- Arithmetic: typology-informed weights for aggregation\n",
    "3. Representation aggregation\n",
    "- A bit what EMEA does, not efficient at inference time\n",
    "    - EMEA even worse as they \"learn\" at inference\n",
    "\n",
    "## 1. Stacks\n",
    "Train a joint language adapter on a group of languages through MLM\n",
    "Here, a distinction could still be made between:\n",
    "- training jointly, no stack\n",
    "    - equal presence of all languages\n",
    "    - weighted presence of all languages\n",
    "- training jointly in a stack with target language adapter on top\n",
    "    - e.g. We already have a \"Romance\" adapter, train \"Asturian\" adapter on top of this\n",
    "- training jointly with a *changing stack*, activating the adapter for the language batch\n",
    "    - What Faisal does?\n",
    "# 2. Parameter aggregation\n",
    "Arithmetic operations on adapters:\n",
    "- adding existing adapters and compare with jointly trained family adapters\n",
    "    - \"average\" of adapters == jointly trained? (cf. Linear mode connectivity)\n",
    "- re-creating typological profile of a language\n",
    "    - preparation step to then \"fine-tune\" on little data (typologically inspired initialization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e387e2aaeaffa02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:11:18.849908Z",
     "start_time": "2025-04-23T20:11:15.160636500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaAdapterModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['heads.default.3.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from adapters import AutoAdapterModel, Stack\n",
    "\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"xlm-roberta-base\")\n",
    "# we load in two adapters\n",
    "model.load_adapter(\"./trained_adapters/mono/de\", load_as=\"de\")\n",
    "model.load_adapter(\"./trained_adapters/mono/en\", load_as=\"en\")\n",
    "# model.load_adapter(\"./trained_adapters/family/en-de-nl-af/mlm\", load_as=\"fam\")\n",
    "\n",
    "model.active_adapters = Stack(\"de\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c4978e09ceb57c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:11:19.808430800Z",
     "start_time": "2025-04-23T20:11:19.794271600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sd = model.state_dict()\n",
    "organized_layers = {}\n",
    "# for each layer:\n",
    "# group 1: layer number\n",
    "# group 2: adapter name\n",
    "# group 3: projection\n",
    "# group 4: projection weight/bias\n",
    "pattern = \"roberta\\.encoder\\.layer\\.([\\d\\w]+)\\.output\\.adapters\\.(\\w+)\\.(\\w+)(?:\\.0)?\\.(\\w+)\"\n",
    "\n",
    "inv_adapters = {}\n",
    "# For invertible adapters\n",
    "# group 1: adapter name\n",
    "# group 2: F/G identifier\n",
    "# group 3: 0/2 layer number\n",
    "# group 4: projection weight/bias\n",
    "inv_pattern = \"roberta\\.invertible_adapters\\.(\\w+)\\.(\\w+)\\.(\\d)\\.(\\w+)\"\n",
    "for key in model.state_dict().keys():\n",
    "    match = re.search(pattern, key)\n",
    "    if match:\n",
    "        layer_num = str(match.group(1))\n",
    "        if layer_num not in organized_layers:\n",
    "            organized_layers[layer_num] = {}\n",
    "        adapter_name = match.group(2)\n",
    "        projection = match.group(3)\n",
    "        projection_type = match.group(4)\n",
    "        # print(f\"Layer: {layer_num}, Adapter: {adapter_name}, Projection: {projection}, Type: {projection_type}\")\n",
    "        if projection not in organized_layers[layer_num]:\n",
    "            organized_layers[layer_num][projection] = {}\n",
    "        if projection_type not in organized_layers[layer_num][projection]:\n",
    "            organized_layers[layer_num][projection][projection_type] = []\n",
    "        organized_layers[layer_num][projection][projection_type].append(key)\n",
    "    inv_match = re.search(inv_pattern, key)\n",
    "    if inv_match:\n",
    "        adapter_name = inv_match.group(1)\n",
    "        identifier = inv_match.group(2)\n",
    "        layer_num = inv_match.group(3)\n",
    "        projection_type = inv_match.group(4)\n",
    "        if identifier not in inv_adapters:\n",
    "            inv_adapters[identifier] = {}\n",
    "        if layer_num not in inv_adapters[identifier]:\n",
    "            inv_adapters[identifier][layer_num] = {}\n",
    "        if projection_type not in inv_adapters[identifier][layer_num]:\n",
    "            inv_adapters[identifier][layer_num][projection_type] = []\n",
    "        inv_adapters[identifier][layer_num][projection_type].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f65f2b3dd8cadf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:36:09.906630300Z",
     "start_time": "2025-04-23T20:36:09.890987500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# we now average the weights and biases of all layers over all adapters\n",
    "new_state_dict = OrderedDict()\n",
    "# to ensure we don't get problems, we check the config of all adapters\n",
    "all_adapters = model.active_adapters\n",
    "config_id = model.adapters_config.adapters[all_adapters[0]]\n",
    "config = model.adapters_config.config_map[config_id]\n",
    "for i in range(1, len(all_adapters)):\n",
    "    config_id = model.adapters_config.adapters[all_adapters[i]]\n",
    "    config_i = model.adapters_config.config_map[config_id]\n",
    "    assert config == config_i, (\n",
    "        f\"Config mismatch: {config} vs {config_i}\\nCurrent methodology only works for same config\"\n",
    "    )\n",
    "\n",
    "# if no problem, we go to the next step\n",
    "for layer_num, projections in organized_layers.items():\n",
    "    for projection, types in projections.items():\n",
    "        for projection_type, keys in types.items():\n",
    "            if projection_type == \"weight\":\n",
    "                # average the weights\n",
    "                # avg_weight = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                avg_weight = (2 / 3) * sd[keys[0]] + (1 / 3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Weight Shape: {avg_weight.shape}\")\n",
    "                if projection == \"adapter_down\":\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.0.weight\"\n",
    "                    ] = avg_weight\n",
    "                else:\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.weight\"\n",
    "                    ] = avg_weight\n",
    "\n",
    "            if projection_type == \"bias\":\n",
    "                # average the biases\n",
    "                # avg_bias = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                avg_bias = (2 / 3) * sd[keys[0]] + (1 / 3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Bias Shape: {avg_bias.shape}\")\n",
    "                if projection == \"adapter_down\":\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.bias\"\n",
    "                    ] = avg_bias\n",
    "                else:\n",
    "                    new_state_dict[\n",
    "                        f\"roberta.encoder.layer.{layer_num}.output.adapters.joined_adapter.{projection}.0.bias\"\n",
    "                    ] = avg_bias\n",
    "for identifier, layer_num in inv_adapters.items():\n",
    "    for layer_num, projections in layer_num.items():\n",
    "        for projection_type, keys in projections.items():\n",
    "            if projection_type == \"weight\":\n",
    "                # average the weights\n",
    "                # avg_weight = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                avg_weight = (2 / 3) * sd[keys[0]] + (1 / 3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Weight Shape: {avg_weight.shape}\")\n",
    "                new_state_dict[f\"roberta.invertible_adapters.{identifier}.F.{layer_num}.{projection_type}\"] = avg_weight\n",
    "            if projection_type == \"bias\":\n",
    "                # average the biases\n",
    "                # avg_bias = sum([sd[key] for key in keys]) / len(keys)\n",
    "                # test: 2/3 \"en\", 1/3 \"de\"\n",
    "                avg_bias = (2 / 3) * sd[keys[0]] + (1 / 3) * sd[keys[1]]\n",
    "                # print(f\"Layer: {layer_num}, Projection: {projection}, Type: {projection_type}, Avg. Bias Shape: {avg_bias.shape}\")\n",
    "                new_state_dict[f\"roberta.invertible_adapters.{identifier}.G.{layer_num}.{projection_type}\"] = avg_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4f9dcba03239b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:36:22.405219100Z",
     "start_time": "2025-04-23T20:36:22.324637500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we have config saved from the last step, we create a new one in the same form\n",
    "if \"joined_adapter\" in model.adapters_config.adapters.keys():\n",
    "    # remove the old one\n",
    "    model.delete_adapter(\"joined_adapter\")\n",
    "model.add_adapter(\"joined_adapter_v2\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bfa3a168ce9b9df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:36:27.857695Z",
     "start_time": "2025-04-23T20:36:27.848445400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    # e.g. \"roberta.encoder.layer.0.output.adapters.joined_adapter.adapter_down.0.weight\"\n",
    "    if \"joined_adapter_v2\" in name and name in new_state_dict:\n",
    "        param.data.copy_(new_state_dict[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "339ea2934154139d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:13:10.316371300Z",
     "start_time": "2025-04-23T20:13:10.309680100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ModuleDict(\n  (de): NICECouplingBlock(\n    (F): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n    (G): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n  )\n  (en): NICECouplingBlock(\n    (F): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n    (G): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n  )\n  (joined_adapter): NICECouplingBlock(\n    (F): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n    (G): Sequential(\n      (0): Linear(in_features=384, out_features=192, bias=True)\n      (1): Activation_Function_Class(\n        (f): ReLU()\n      )\n      (2): Linear(in_features=192, out_features=384, bias=True)\n    )\n  )\n)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.roberta.invertible_adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ef990023bdf7aa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:36:39.126653400Z",
     "start_time": "2025-04-23T20:36:39.091432300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_adapter(\"./trained_adapters/mono/joined_adapter_v2\", \"joined_adapter_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c10add83e138ca23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:14:41.801435Z",
     "start_time": "2025-04-23T20:14:41.796737500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we evaluated the adapter (along with de and en) on ner in another script\n",
    "import json\n",
    "\n",
    "results = json.load(open(\"methods/eval_dict_joined.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "116e1fa2605c1c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T20:14:46.511129300Z",
     "start_time": "2025-04-23T20:14:46.507374900Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_loss, avg en/de: 0.467184379696846, joined: 0.4572905898094177\n",
      "eval_model_preparation_time, avg en/de: 0.0086, joined: 0.006\n",
      "eval_precision, avg en/de: 0.5284809848704373, joined: 0.5575268817204301\n",
      "eval_recall, avg en/de: 0.7186234817813766, joined: 0.6997300944669366\n",
      "eval_f1, avg en/de: 0.6085899656003557, joined: 0.6205864751645721\n",
      "eval_accuracy, avg en/de: 0.8567322573513155, joined: 0.8606566438204731\n",
      "eval_runtime, avg en/de: 4.626099999999999, joined: 4.6817\n",
      "eval_samples_per_second, avg en/de: 216.16500000000002, joined: 213.599\n",
      "eval_steps_per_second, avg en/de: 27.0205, joined: 26.7\n"
     ]
    }
   ],
   "source": [
    "for (name, de), (_, en), (_, joined) in zip(\n",
    "    results[\"de\"].items(), results[\"en\"].items(), results[\"joined_adapter\"].items()\n",
    "):\n",
    "    print(f\"{name}, avg en/de: {(en + de) / 2}, joined: {joined}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27057c04818768d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Copying the approach from language-arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a03cfa79ad14e664",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:07:50.069731600Z",
     "start_time": "2025-04-23T21:07:49.414327600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n",
      "XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "~\n",
      " XLMRobertaLayer(\n",
      "  (attention): XLMRobertaAttention(\n",
      "    (self): XLMRobertaSdpaSelfAttentionWithAdapters(\n",
      "      (query): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (key): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (value): LoRALinearTorch(\n",
      "        in_features=768, out_features=768, bias=True\n",
      "        (loras): ModuleDict()\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (prefix_tuning): PrefixTuningLayer(\n",
      "        (prefix_gates): ModuleDict()\n",
      "        (pool): PrefixTuningPool(\n",
      "          (prefix_tunings): ModuleDict()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (output): XLMRobertaSelfOutputWithAdapters(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (adapters): ModuleDict()\n",
      "      (adapter_fusion_layer): ModuleDict()\n",
      "    )\n",
      "  )\n",
      "  (intermediate): XLMRobertaIntermediate(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=768, out_features=3072, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (intermediate_act_fn): GELUActivation()\n",
      "  )\n",
      "  (output): XLMRobertaOutputWithAdapters(\n",
      "    (dense): LoRALinearTorch(\n",
      "      in_features=3072, out_features=768, bias=True\n",
      "      (loras): ModuleDict()\n",
      "    )\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (adapters): ModuleDict(\n",
      "      (de): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (en): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (joined_adapter_v2): Adapter(\n",
      "        (non_linearity): Activation_Function_Class(\n",
      "          (f): ReLU()\n",
      "        )\n",
      "        (adapter_down): Sequential(\n",
      "          (0): Linear(in_features=768, out_features=384, bias=True)\n",
      "          (1): Activation_Function_Class(\n",
      "            (f): ReLU()\n",
      "          )\n",
      "        )\n",
      "        (adapter_up): Linear(in_features=384, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (adapter_fusion_layer): ModuleDict()\n",
      "  )\n",
      "  (reft_layer): ReftLayer(\n",
      "    (refts): ModuleDict()\n",
      "  )\n",
      ") \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "model1 = copy.deepcopy(model.cpu())\n",
    "model2 = copy.deepcopy(model.cpu())\n",
    "lang1 = \"de\"\n",
    "lang2 = \"en\"\n",
    "\n",
    "\n",
    "tgt = \"j_adapter\"\n",
    "\n",
    "\n",
    "layers1 = model1.roberta.encoder.layer\n",
    "layers2 = model2.roberta.encoder.layer\n",
    "for l1, l2 in zip(layers1, layers2):\n",
    "    print(l1, \"\\n~\\n\", l2, \"\\n\" + \"-\" * 50)\n",
    "    adapter1 = l1.output.adapters[lang1]\n",
    "    adapter2 = l2.output.adapters[lang2]\n",
    "    l1.output.adapters = nn.ModuleDict()\n",
    "    l2.output.adapters = nn.ModuleDict()\n",
    "    l1.output.adapters[tgt] = adapter1\n",
    "    l2.output.adapters[tgt] = adapter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a4fc81536dad3c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T21:05:34.031298700Z",
     "start_time": "2025-04-23T21:05:34.024283Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.roberta.encoder.layer[0].output.adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ecc3812d3b6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
